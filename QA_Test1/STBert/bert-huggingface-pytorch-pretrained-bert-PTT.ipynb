{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "colab": {
   "name": "bert-huggingface-pytorch-pretrained-bert-PTT.ipynb",
   "provenance": []
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "#pip install tqdm boto3 requests regex\n",
    "#pip install pycuda\n",
    "#pip install pytorch_pretrained_bert==0.4.0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1b60abf89faaf45cc5965f4caecab62c3cdbc62b",
    "id": "un0DeZx_uFwQ",
    "colab_type": "text"
   },
   "source": [
    "# Just list which models can used\n",
    "\n",
    "            'bert-base-uncased'\n",
    "            'bert-large-uncased'\n",
    "            'bert-base-cased': \n",
    "            'bert-large-cased'\n",
    "            'bert-base-multilingual-uncased'\n",
    "            'bert-base-multilingual-cased'\n",
    "            'bert-base-chinese'"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "0LpeIkjNuFwX",
    "colab_type": "code",
    "outputId": "b4d49997-78cf-4670-e84c-6968d0956376",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1577892081345,
     "user_tz": -480,
     "elapsed": 145738,
     "user": {
      "displayName": "曹錫璋",
      "photoUrl": "",
      "userId": "12089102222035734305"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    }
   },
   "source": [
    "import torch\n",
    "import pycuda.driver as cuda\n",
    "cuda.init()\n",
    "## Get Id of default device\n",
    "\n",
    "torch.cuda.current_device()\n",
    "# 0\n",
    "cuda.Device(0).name() #"
   ],
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "'GeForce RTX 2060'"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "MIrV9IoLuFwe",
    "colab_type": "code",
    "outputId": "c55ab8fe-8bb4-4320-962d-e47aa800a186",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1577892081346,
     "user_tz": -480,
     "elapsed": 145723,
     "user": {
      "displayName": "曹錫璋",
      "photoUrl": "",
      "userId": "12089102222035734305"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    }
   },
   "source": [
    "torch.cuda.get_device_name(0)"
   ],
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "'GeForce RTX 2060'"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "EjdE1JtZuFwh",
    "colab_type": "code",
    "outputId": "12dbb377-c7c9-4044-ccb9-a1d7dd886334",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1577892081348,
     "user_tz": -480,
     "elapsed": 145707,
     "user": {
      "displayName": "曹錫璋",
      "photoUrl": "",
      "userId": "12089102222035734305"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    }
   },
   "source": [
    "# A simple class to know about your cuda devices\n",
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit # Necessary for using its functions\n",
    "cuda.init() # Necesarry for using its functions\n",
    "\n",
    "class aboutCudaDevices():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def num_devices(self):\n",
    "        \"\"\"返回 cuda 设备的数量\"\"\"\n",
    "        return cuda.Device.count()\n",
    "    \n",
    "    def devices(self):\n",
    "        \"\"\"获取所有可用的设备的名称\"\"\"\n",
    "        num = cuda.Device.count()\n",
    "        print(\"%d device(s) found:\"%num)\n",
    "        for i in range(num):\n",
    "            print(cuda.Device(i).name(), \"(Id: %d)\"%i)\n",
    "            \n",
    "    def mem_info(self):\n",
    "        \"\"\"获取所有设备的总内存和可用内存\"\"\"\n",
    "        available, total = cuda.mem_get_info()\n",
    "        print(\"Available: %.2f GB\\nTotal:     %.2f GB\"%(available/1e9, total/1e9))\n",
    "        \n",
    "    def attributes(self, device_id=0):\n",
    "        \"\"\"返回指定 id 的设备的属性信息\"\"\"\n",
    "        return cuda.Device(device_id).get_attributes()\n",
    "    \n",
    "    def __repr__(self):\n",
    "        \"\"\"输出设备的数量和其id、内存信息\"\"\"\n",
    "        num = cuda.Device.count()\n",
    "        string = \"\"\n",
    "        string += (\"%d device(s) found:\\n\"%num)\n",
    "        for i in range(num):\n",
    "            string += ( \"    %d) %s (Id: %d)\\n\"%((i+1),cuda.Device(i).name(),i))\n",
    "            string += (\"          Memory: %.2f GB\\n\"%(cuda.Device(i).total_memory()/1e9))\n",
    "        return string\n",
    "\n",
    "# You can print output just by typing its name (__repr__):\n",
    "aboutCudaDevices()\n",
    "# 1 device(s) found:\n",
    "#    1) Tesla K80 (Id: 0)\n",
    "#          Memory: 12.00 GB\n"
   ],
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "1 device(s) found:\n    1) GeForce RTX 2060 (Id: 0)\n          Memory: 6.44 GB"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "5OTIhNiywRon",
    "colab_type": "code",
    "colab": {}
   },
   "source": [],
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "_uuid": "7bc99e8cf9434bc63e4941cce3aec071dc83105e",
    "id": "hNctQ_BouFwn",
    "colab_type": "code",
    "outputId": "5a471b37-ac11-4154-b193-a1840ab5d669",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1577892081557,
     "user_tz": -480,
     "elapsed": 145895,
     "user": {
      "displayName": "曹錫璋",
      "photoUrl": "",
      "userId": "12089102222035734305"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    }
   },
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import zipfile\n",
    "import datetime\n",
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM, BertForSequenceClassification\n",
    "\n",
    "VOCAB ='./CQA/data/bert-base-chinese-vocab.txt'\n",
    "MODEL ='./CQA/data/pytorch_model'"
   ],
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "_uuid": "6193344fe927573b3f67da350021854d7e282ea9",
    "id": "4hLwNt8xuFws",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "outputId": "cdaac764-c5d9-4e8a-ce9c-d18ffa5d4ed9",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1577892081854,
     "user_tz": -480,
     "elapsed": 146175,
     "user": {
      "displayName": "曹錫璋",
      "photoUrl": "",
      "userId": "12089102222035734305"
     }
    }
   },
   "source": [
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')"
   ],
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "_uuid": "600edbf529304d553c9db781e5e6bd1bbcf17108",
    "id": "U0Z3fhp2uFwv",
    "colab_type": "code",
    "outputId": "cdff6f5a-2ba7-4bd0-8cba-a2fc79fb0c01",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1577892116699,
     "user_tz": -480,
     "elapsed": 180975,
     "user": {
      "displayName": "曹錫璋",
      "photoUrl": "",
      "userId": "12089102222035734305"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 142
    }
   },
   "source": [
    "if os.path.isdir(\"./CQA/data/\"):\n",
    "    TRAIN_CSV_PATH = './CQA/data/train.csv'\n",
    "    TEST_CSV_PATH = './CQA/data/test.csv'\n",
    "    TOKENIZED_TRAIN_CSV_PATH = \"./CQA/data/tokenized_train2.csv\"\n",
    "else:\n",
    "    TRAIN_CSV_PATH = './CQA/data/train.csv'\n",
    "    TEST_CSV_PATH = './CQA/data/test.csv'\n",
    "    TOKENIZED_TRAIN_CSV_PATH = \"\"\n",
    "    \n",
    "train = pd.read_csv(TRAIN_CSV_PATH)\n",
    "test = pd.read_csv(TEST_CSV_PATH)\n",
    "cols = ['titletext',\n",
    "        'label']\n",
    "train = train.loc[:, cols]\n",
    "test = test.loc[:, cols]\n",
    "train.fillna('UNKNOWN', inplace=True)\n",
    "test.fillna('UNKNOWN', inplace=True)\n",
    "train.head(3)"
   ],
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "                                           titletext label\n0  嚴國安於民國106年10月26日20時10分許，在高雄市鳳山區鳳林路之黃埔公園公車站牌處，見...    搶奪\n1  艾康淇前因承攬林徐湘楹（已成年）之房屋裝修工程，並因而發生民事紛爭而在本院涉訟，由本院以10...  妨害名譽\n2  黃淑英與黃周招君為母女關係，黃淑英明知其母黃周招君欲將其名下所有、如附表所示之房地贈與予黃淑...  偽造文書",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>titletext</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>嚴國安於民國106年10月26日20時10分許，在高雄市鳳山區鳳林路之黃埔公園公車站牌處，見...</td>\n      <td>搶奪</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>艾康淇前因承攬林徐湘楹（已成年）之房屋裝修工程，並因而發生民事紛爭而在本院涉訟，由本院以10...</td>\n      <td>妨害名譽</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>黃淑英與黃周招君為母女關係，黃淑英明知其母黃周招君欲將其名下所有、如附表所示之房地贈與予黃淑...</td>\n      <td>偽造文書</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "_uuid": "04b5fcaff32c0e8bf13c586b6c4462a6cb00d7bc",
    "id": "gValzflauFw5",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "from sklearn.model_selection \\\n",
    "    import train_test_split\n",
    "\n",
    "VALIDATION_RATIO = 0.1\n",
    "\n",
    "RANDOM_STATE = 9527\n",
    "\n",
    "train, val= \\\n",
    "    train_test_split(\n",
    "        train, \n",
    "        test_size=VALIDATION_RATIO, \n",
    "        random_state=RANDOM_STATE\n",
    ")"
   ],
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "_uuid": "0a86c71b6d2375d5e36be524dcffe634b3f6d3be",
    "id": "ZSmfACgQuFw-",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "'''\n",
    "Counter({'BabyMother': 5009,\n",
    "         'Baseball': 5017,\n",
    "         'C_Chat': 5007,\n",
    "         'EAseries': 5009,\n",
    "         'HatePolitics': 5013,\n",
    "         'Japan_Travel': 5009,\n",
    "         'KoreaStar': 5001,\n",
    "         'Lifeismoney': 5006,\n",
    "         'LoL': 5007,\n",
    "         'MobileComm': 5009,\n",
    "         'NBA': 5003,\n",
    "         'PC_Shopping': 5001,\n",
    "         'Stock': 5008,\n",
    "         'Tech_Job': 5002,\n",
    "         'Tennis': 5017,\n",
    "         'car': 5009,\n",
    "         'creditcard': 5001,\n",
    "         'e-shopping': 5010,\n",
    "         'marvel': 5014,\n",
    "         'movie': 5009})\n",
    "'''\n",
    "label_list = [\n",
    "'傷害',\n",
    "'侵占',\n",
    "'公共危險',\n",
    "'過失傷害',\n",
    "'妨害自由',\n",
    "'妨害公務',\n",
    "'竊盜',\n",
    "'妨害名譽',\n",
    "'妨害性自主',\n",
    "'偽造文書',\n",
    "'搶奪',\n",
    "'毀損',\n",
    "'妨害風化',\n",
    "'毒品危害防制',\n",
    "'妨害性自主罪',\n",
    "'強盜',\n",
    "'著作權法',\n",
    "'其他',\n",
    "]"
   ],
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "_uuid": "6193fe7713ff9a71cb8153a16e76819c18c3cf03",
    "id": "OusFJ8_ruFxC",
    "colab_type": "code",
    "outputId": "de902e20-e1e4-450f-e693-4df2fea5899e",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1577892140518,
     "user_tz": -480,
     "elapsed": 11314,
     "user": {
      "displayName": "曹錫璋",
      "photoUrl": "",
      "userId": "12089102222035734305"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    }
   },
   "source": [
    "from run_classifier import *\n",
    "\n",
    "train_examples = [InputExample('train', row.titletext, None, row.label) for row in train.itertuples()]\n",
    "val_examples = [InputExample('val', row.titletext, None, row.label) for row in val.itertuples()]\n",
    "test_examples = [InputExample('test', row.titletext, None, 'None') for row in test.itertuples()]\n",
    "\n",
    "len(train_examples)"
   ],
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "1204"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "_uuid": "75aa53708b72e3992990f06862a98c79299a325a",
    "id": "ktSbAxwYuFxG",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "orginal_total = len(train_examples)\n",
    "train_examples = train_examples[:int(orginal_total*0.8)]"
   ],
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "_uuid": "ab0b13dd30fe5dd3da025d961e288bd4431075d5",
    "id": "WFkYbje_uFxM",
    "colab_type": "code",
    "outputId": "92b671bc-d6fe-4b01-a816-b4c279b877f8",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1577892140519,
     "user_tz": -480,
     "elapsed": 11295,
     "user": {
      "displayName": "曹錫璋",
      "photoUrl": "",
      "userId": "12089102222035734305"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    }
   },
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = torch.device(\"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "print(n_gpu)\n",
    "gradient_accumulation_steps = 1\n",
    "train_batch_size = 2#32\n",
    "eval_batch_size = 2#128\n",
    "train_batch_size = train_batch_size // gradient_accumulation_steps\n",
    "output_dir = './CQA/data/output'\n",
    "bert_model = 'bert-base-chinese'\n",
    "num_train_epochs = 3\n",
    "num_train_optimization_steps = int(\n",
    "            len(train_examples) / train_batch_size / gradient_accumulation_steps) * num_train_epochs\n",
    "cache_dir = \"model\"\n",
    "learning_rate = 5e-5\n",
    "warmup_proportion = 0.1\n",
    "max_seq_length = 512\n",
    "label_list = [\n",
    "'傷害',\n",
    "'侵占',\n",
    "'公共危險',\n",
    "'過失傷害',\n",
    "'妨害自由',\n",
    "'妨害公務',\n",
    "'竊盜',\n",
    "'妨害名譽',\n",
    "'妨害性自主',\n",
    "'偽造文書',\n",
    "'搶奪',\n",
    "'毀損',\n",
    "'妨害風化',\n",
    "'毒品危害防制',\n",
    "'妨害性自主罪',\n",
    "'強盜',\n",
    "'著作權法',\n",
    "'其他',\n",
    "]"
   ],
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    }
   ],
   "source": [
    "print(len(label_list))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "_kg_hide-output": true,
    "_uuid": "28d44f3b50b9cef652ec7ffc2822df82b75293df",
    "scrolled": true,
    "id": "T-7ANqEvuFxU",
    "colab_type": "code",
    "outputId": "5bd460c8-b33f-4900-9fa5-e63923b194ae",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1577892156561,
     "user_tz": -480,
     "elapsed": 27328,
     "user": {
      "displayName": "曹錫璋",
      "photoUrl": "",
      "userId": "12089102222035734305"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 547
    }
   },
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(bert_model)\n",
    "model = BertForSequenceClassification.from_pretrained(bert_model,\n",
    "              cache_dir=cache_dir,\n",
    "              num_labels = len(label_list))"
   ],
   "execution_count": 14,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/03/2020 17:58:06 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-vocab.txt from cache at C:\\Users\\user\\.pytorch_pretrained_bert\\8a0c070123c1f794c42a29c6904beb7c1b8715741e235bee04aca2c7636fc83f.9b42061518a39ca00b8b52059fd2bede8daa613f8a8671500e518a8c29de8c00\n",
      "10/03/2020 17:58:07 - INFO - pytorch_pretrained_bert.modeling -   loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese.tar.gz from cache at model\\42d4a64dda3243ffeca7ec268d5544122e67d9d06b971608796b483925716512.02ac7d664cff08d793eb00d6aac1d04368a1322435e5fe0a27c70b0b3a85327f\n",
      "10/03/2020 17:58:07 - INFO - pytorch_pretrained_bert.modeling -   extracting archive file model\\42d4a64dda3243ffeca7ec268d5544122e67d9d06b971608796b483925716512.02ac7d664cff08d793eb00d6aac1d04368a1322435e5fe0a27c70b0b3a85327f to temp dir C:\\Users\\user\\AppData\\Local\\Temp\\tmpwefhler0\n",
      "10/03/2020 17:58:10 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "10/03/2020 17:58:12 - INFO - pytorch_pretrained_bert.modeling -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "10/03/2020 17:58:12 - INFO - pytorch_pretrained_bert.modeling -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "DjdcRYLsuFxZ",
    "colab_type": "code",
    "outputId": "80de235b-953c-4fe6-85c9-6a72c728da6b",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1577892169066,
     "user_tz": -480,
     "elapsed": 39824,
     "user": {
      "displayName": "曹錫璋",
      "photoUrl": "",
      "userId": "12089102222035734305"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    }
   },
   "source": [
    "model.to(device)\n",
    "if n_gpu > 1:\n",
    "    model = torch.nn.DataParallel(model)\n",
    "model, tokenizer"
   ],
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "(BertForSequenceClassification(\n   (bert): BertModel(\n     (embeddings): BertEmbeddings(\n       (word_embeddings): Embedding(21128, 768, padding_idx=0)\n       (position_embeddings): Embedding(512, 768)\n       (token_type_embeddings): Embedding(2, 768)\n       (LayerNorm): BertLayerNorm()\n       (dropout): Dropout(p=0.1, inplace=False)\n     )\n     (encoder): BertEncoder(\n       (layer): ModuleList(\n         (0): BertLayer(\n           (attention): BertAttention(\n             (self): BertSelfAttention(\n               (query): Linear(in_features=768, out_features=768, bias=True)\n               (key): Linear(in_features=768, out_features=768, bias=True)\n               (value): Linear(in_features=768, out_features=768, bias=True)\n               (dropout): Dropout(p=0.1, inplace=False)\n             )\n             (output): BertSelfOutput(\n               (dense): Linear(in_features=768, out_features=768, bias=True)\n               (LayerNorm): BertLayerNorm()\n               (dropout): Dropout(p=0.1, inplace=False)\n             )\n           )\n           (intermediate): BertIntermediate(\n             (dense): Linear(in_features=768, out_features=3072, bias=True)\n           )\n           (output): BertOutput(\n             (dense): Linear(in_features=3072, out_features=768, bias=True)\n             (LayerNorm): BertLayerNorm()\n             (dropout): Dropout(p=0.1, inplace=False)\n           )\n         )\n         (1): BertLayer(\n           (attention): BertAttention(\n             (self): BertSelfAttention(\n               (query): Linear(in_features=768, out_features=768, bias=True)\n               (key): Linear(in_features=768, out_features=768, bias=True)\n               (value): Linear(in_features=768, out_features=768, bias=True)\n               (dropout): Dropout(p=0.1, inplace=False)\n             )\n             (output): BertSelfOutput(\n               (dense): Linear(in_features=768, out_features=768, bias=True)\n               (LayerNorm): BertLayerNorm()\n               (dropout): Dropout(p=0.1, inplace=False)\n             )\n           )\n           (intermediate): BertIntermediate(\n             (dense): Linear(in_features=768, out_features=3072, bias=True)\n           )\n           (output): BertOutput(\n             (dense): Linear(in_features=3072, out_features=768, bias=True)\n             (LayerNorm): BertLayerNorm()\n             (dropout): Dropout(p=0.1, inplace=False)\n           )\n         )\n         (2): BertLayer(\n           (attention): BertAttention(\n             (self): BertSelfAttention(\n               (query): Linear(in_features=768, out_features=768, bias=True)\n               (key): Linear(in_features=768, out_features=768, bias=True)\n               (value): Linear(in_features=768, out_features=768, bias=True)\n               (dropout): Dropout(p=0.1, inplace=False)\n             )\n             (output): BertSelfOutput(\n               (dense): Linear(in_features=768, out_features=768, bias=True)\n               (LayerNorm): BertLayerNorm()\n               (dropout): Dropout(p=0.1, inplace=False)\n             )\n           )\n           (intermediate): BertIntermediate(\n             (dense): Linear(in_features=768, out_features=3072, bias=True)\n           )\n           (output): BertOutput(\n             (dense): Linear(in_features=3072, out_features=768, bias=True)\n             (LayerNorm): BertLayerNorm()\n             (dropout): Dropout(p=0.1, inplace=False)\n           )\n         )\n         (3): BertLayer(\n           (attention): BertAttention(\n             (self): BertSelfAttention(\n               (query): Linear(in_features=768, out_features=768, bias=True)\n               (key): Linear(in_features=768, out_features=768, bias=True)\n               (value): Linear(in_features=768, out_features=768, bias=True)\n               (dropout): Dropout(p=0.1, inplace=False)\n             )\n             (output): BertSelfOutput(\n               (dense): Linear(in_features=768, out_features=768, bias=True)\n               (LayerNorm): BertLayerNorm()\n               (dropout): Dropout(p=0.1, inplace=False)\n             )\n           )\n           (intermediate): BertIntermediate(\n             (dense): Linear(in_features=768, out_features=3072, bias=True)\n           )\n           (output): BertOutput(\n             (dense): Linear(in_features=3072, out_features=768, bias=True)\n             (LayerNorm): BertLayerNorm()\n             (dropout): Dropout(p=0.1, inplace=False)\n           )\n         )\n         (4): BertLayer(\n           (attention): BertAttention(\n             (self): BertSelfAttention(\n               (query): Linear(in_features=768, out_features=768, bias=True)\n               (key): Linear(in_features=768, out_features=768, bias=True)\n               (value): Linear(in_features=768, out_features=768, bias=True)\n               (dropout): Dropout(p=0.1, inplace=False)\n             )\n             (output): BertSelfOutput(\n               (dense): Linear(in_features=768, out_features=768, bias=True)\n               (LayerNorm): BertLayerNorm()\n               (dropout): Dropout(p=0.1, inplace=False)\n             )\n           )\n           (intermediate): BertIntermediate(\n             (dense): Linear(in_features=768, out_features=3072, bias=True)\n           )\n           (output): BertOutput(\n             (dense): Linear(in_features=3072, out_features=768, bias=True)\n             (LayerNorm): BertLayerNorm()\n             (dropout): Dropout(p=0.1, inplace=False)\n           )\n         )\n         (5): BertLayer(\n           (attention): BertAttention(\n             (self): BertSelfAttention(\n               (query): Linear(in_features=768, out_features=768, bias=True)\n               (key): Linear(in_features=768, out_features=768, bias=True)\n               (value): Linear(in_features=768, out_features=768, bias=True)\n               (dropout): Dropout(p=0.1, inplace=False)\n             )\n             (output): BertSelfOutput(\n               (dense): Linear(in_features=768, out_features=768, bias=True)\n               (LayerNorm): BertLayerNorm()\n               (dropout): Dropout(p=0.1, inplace=False)\n             )\n           )\n           (intermediate): BertIntermediate(\n             (dense): Linear(in_features=768, out_features=3072, bias=True)\n           )\n           (output): BertOutput(\n             (dense): Linear(in_features=3072, out_features=768, bias=True)\n             (LayerNorm): BertLayerNorm()\n             (dropout): Dropout(p=0.1, inplace=False)\n           )\n         )\n         (6): BertLayer(\n           (attention): BertAttention(\n             (self): BertSelfAttention(\n               (query): Linear(in_features=768, out_features=768, bias=True)\n               (key): Linear(in_features=768, out_features=768, bias=True)\n               (value): Linear(in_features=768, out_features=768, bias=True)\n               (dropout): Dropout(p=0.1, inplace=False)\n             )\n             (output): BertSelfOutput(\n               (dense): Linear(in_features=768, out_features=768, bias=True)\n               (LayerNorm): BertLayerNorm()\n               (dropout): Dropout(p=0.1, inplace=False)\n             )\n           )\n           (intermediate): BertIntermediate(\n             (dense): Linear(in_features=768, out_features=3072, bias=True)\n           )\n           (output): BertOutput(\n             (dense): Linear(in_features=3072, out_features=768, bias=True)\n             (LayerNorm): BertLayerNorm()\n             (dropout): Dropout(p=0.1, inplace=False)\n           )\n         )\n         (7): BertLayer(\n           (attention): BertAttention(\n             (self): BertSelfAttention(\n               (query): Linear(in_features=768, out_features=768, bias=True)\n               (key): Linear(in_features=768, out_features=768, bias=True)\n               (value): Linear(in_features=768, out_features=768, bias=True)\n               (dropout): Dropout(p=0.1, inplace=False)\n             )\n             (output): BertSelfOutput(\n               (dense): Linear(in_features=768, out_features=768, bias=True)\n               (LayerNorm): BertLayerNorm()\n               (dropout): Dropout(p=0.1, inplace=False)\n             )\n           )\n           (intermediate): BertIntermediate(\n             (dense): Linear(in_features=768, out_features=3072, bias=True)\n           )\n           (output): BertOutput(\n             (dense): Linear(in_features=3072, out_features=768, bias=True)\n             (LayerNorm): BertLayerNorm()\n             (dropout): Dropout(p=0.1, inplace=False)\n           )\n         )\n         (8): BertLayer(\n           (attention): BertAttention(\n             (self): BertSelfAttention(\n               (query): Linear(in_features=768, out_features=768, bias=True)\n               (key): Linear(in_features=768, out_features=768, bias=True)\n               (value): Linear(in_features=768, out_features=768, bias=True)\n               (dropout): Dropout(p=0.1, inplace=False)\n             )\n             (output): BertSelfOutput(\n               (dense): Linear(in_features=768, out_features=768, bias=True)\n               (LayerNorm): BertLayerNorm()\n               (dropout): Dropout(p=0.1, inplace=False)\n             )\n           )\n           (intermediate): BertIntermediate(\n             (dense): Linear(in_features=768, out_features=3072, bias=True)\n           )\n           (output): BertOutput(\n             (dense): Linear(in_features=3072, out_features=768, bias=True)\n             (LayerNorm): BertLayerNorm()\n             (dropout): Dropout(p=0.1, inplace=False)\n           )\n         )\n         (9): BertLayer(\n           (attention): BertAttention(\n             (self): BertSelfAttention(\n               (query): Linear(in_features=768, out_features=768, bias=True)\n               (key): Linear(in_features=768, out_features=768, bias=True)\n               (value): Linear(in_features=768, out_features=768, bias=True)\n               (dropout): Dropout(p=0.1, inplace=False)\n             )\n             (output): BertSelfOutput(\n               (dense): Linear(in_features=768, out_features=768, bias=True)\n               (LayerNorm): BertLayerNorm()\n               (dropout): Dropout(p=0.1, inplace=False)\n             )\n           )\n           (intermediate): BertIntermediate(\n             (dense): Linear(in_features=768, out_features=3072, bias=True)\n           )\n           (output): BertOutput(\n             (dense): Linear(in_features=3072, out_features=768, bias=True)\n             (LayerNorm): BertLayerNorm()\n             (dropout): Dropout(p=0.1, inplace=False)\n           )\n         )\n         (10): BertLayer(\n           (attention): BertAttention(\n             (self): BertSelfAttention(\n               (query): Linear(in_features=768, out_features=768, bias=True)\n               (key): Linear(in_features=768, out_features=768, bias=True)\n               (value): Linear(in_features=768, out_features=768, bias=True)\n               (dropout): Dropout(p=0.1, inplace=False)\n             )\n             (output): BertSelfOutput(\n               (dense): Linear(in_features=768, out_features=768, bias=True)\n               (LayerNorm): BertLayerNorm()\n               (dropout): Dropout(p=0.1, inplace=False)\n             )\n           )\n           (intermediate): BertIntermediate(\n             (dense): Linear(in_features=768, out_features=3072, bias=True)\n           )\n           (output): BertOutput(\n             (dense): Linear(in_features=3072, out_features=768, bias=True)\n             (LayerNorm): BertLayerNorm()\n             (dropout): Dropout(p=0.1, inplace=False)\n           )\n         )\n         (11): BertLayer(\n           (attention): BertAttention(\n             (self): BertSelfAttention(\n               (query): Linear(in_features=768, out_features=768, bias=True)\n               (key): Linear(in_features=768, out_features=768, bias=True)\n               (value): Linear(in_features=768, out_features=768, bias=True)\n               (dropout): Dropout(p=0.1, inplace=False)\n             )\n             (output): BertSelfOutput(\n               (dense): Linear(in_features=768, out_features=768, bias=True)\n               (LayerNorm): BertLayerNorm()\n               (dropout): Dropout(p=0.1, inplace=False)\n             )\n           )\n           (intermediate): BertIntermediate(\n             (dense): Linear(in_features=768, out_features=3072, bias=True)\n           )\n           (output): BertOutput(\n             (dense): Linear(in_features=3072, out_features=768, bias=True)\n             (LayerNorm): BertLayerNorm()\n             (dropout): Dropout(p=0.1, inplace=False)\n           )\n         )\n       )\n     )\n     (pooler): BertPooler(\n       (dense): Linear(in_features=768, out_features=768, bias=True)\n       (activation): Tanh()\n     )\n   )\n   (dropout): Dropout(p=0.1, inplace=False)\n   (classifier): Linear(in_features=768, out_features=18, bias=True)\n ), <pytorch_pretrained_bert.tokenization.BertTokenizer at 0x28733995048>)"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "_uuid": "b93a5f7d8ae1012a7e606e9d1581722a4a2e329e",
    "id": "V3VGxJGQuFxe",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# Prepare optimizer\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "optimizer = BertAdam(optimizer_grouped_parameters,\n",
    "                             lr=learning_rate,\n",
    "                             warmup=warmup_proportion,\n",
    "                             t_total=num_train_optimization_steps)"
   ],
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "NBzxHEAHuFxg",
    "colab_type": "code",
    "colab": {}
   },
   "source": [],
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "_uuid": "b098b3962887d0bede5052b4cf25902d9fc5c4fd",
    "id": "ctYh8KRwuFxi",
    "colab_type": "code",
    "outputId": "1df9a842-c09c-4eb2-edbf-03c19a243212",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1577892178205,
     "user_tz": -480,
     "elapsed": 48934,
     "user": {
      "displayName": "曹錫璋",
      "photoUrl": "",
      "userId": "12089102222035734305"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    }
   },
   "source": [
    "\n",
    "global_step = 0\n",
    "nb_tr_steps = 0\n",
    "tr_loss = 0\n",
    "\n",
    "train_features = convert_examples_to_features(\n",
    "    train_examples, label_list, max_seq_length, tokenizer)\n",
    "logger.info(\"***** Running training *****\")\n",
    "logger.info(\"  Num examples = %d\", len(train_examples))\n",
    "logger.info(\"  Batch size = %d\", train_batch_size)\n",
    "logger.info(\"  Num steps = %d\", num_train_optimization_steps)\n",
    "all_input_ids = torch.tensor([f.input_ids for f in train_features], dtype=torch.long)\n",
    "all_input_mask = torch.tensor([f.input_mask for f in train_features], dtype=torch.long)\n",
    "all_segment_ids = torch.tensor([f.segment_ids for f in train_features], dtype=torch.long)\n",
    "all_label_ids = torch.tensor([f.label_id for f in train_features], dtype=torch.long)\n",
    "\n",
    "train_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=train_batch_size)\n",
    "\n",
    "model.train()\n",
    "\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
    "\n",
    "# Print optimizer's state_dict\n",
    "print(\"Optimizer's state_dict:\")\n",
    "for var_name in optimizer.state_dict():\n",
    "    print(var_name, \"\\t\", optimizer.state_dict()[var_name])\n",
    "    \n",
    "'''\n",
    "torch.save(model.state_dict(), 'BertModel')\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "model = torch.load(\"BertModel\", map_location=torch.device('cpu'))\n",
    "'''"
   ],
   "execution_count": 17,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/03/2020 17:58:13 - INFO - run_classifier -   *** Example ***\n",
      "10/03/2020 17:58:13 - INFO - run_classifier -   guid: train\n",
      "10/03/2020 17:58:13 - INFO - run_classifier -   tokens: [CLS] 查 被 告 周 柏 宏 因 違 反 毒 品 危 害 防 制 條 例 案 件 ， 於 偵 查 中 經 具 保 人 周 竹 良 依 臺 灣 新 北 地 方 法 院 檢 察 署 檢 察 官 指 定 保 證 金 新 臺 幣 5 萬 元 ， 而 提 出 現 金 具 保 釋 放 在 案 ， 有 臺 灣 新 北 地 方 法 院 檢 察 署 刑 事 保 證 金 收 據 1 紙 在 卷 可 佐 （ 見 臺 灣 新 北 地 方 法 院 檢 察 署 103 年 度 偵 字 第 186 ##26 號 偵 查 卷 第 266 頁 背 面 ） ， 茲 因 被 告 於 本 院 104 年 度 原 訴 字 第 40 號 毒 品 危 害 防 制 條 例 案 件 準 備 程 序 時 ， 經 合 法 傳 喚 ， 並 通 知 具 保 人 督 促 被 告 到 庭 應 訊 ， 被 告 無 正 當 理 由 而 未 遵 期 到 庭 ， 復 經 本 院 拘 提 無 著 ； 又 具 保 人 經 通 知 亦 未 遵 期 通 知 或 帶 同 被 告 到 案 接 受 應 訊 等 情 ， 此 有 本 院 送 達 證 書 3 份 、 拘 提 報 告 、 具 保 人 之 戶 役 政 連 結 作 業 系 統 查 詢 資 料 各 1 份 附 卷 可 憑 （ 見 本 院 卷 第 38 頁 、 第 40 至 41 頁 、 第 98 頁 、 第 52 頁 ） 。 又 被 告 未 受 羈 押 及 在 監 執 行 ， 顯 已 逃 匿 等 情 ， 復 經 本 院 依 職 權 查 明 屬 實 ， 亦 有 臺 灣 高 等 法 院 被 告 在 監 在 押 全 國 紀 錄 表 1 份 附 卷 足 憑 。 揆 諸 前 揭 說 明 ， 自 應 將 具 保 人 原 繳 納 之 上 開 保 證 金 及 實 收 利 息 沒 入 之 。 [SEP]\n",
      "10/03/2020 17:58:13 - INFO - run_classifier -   input_ids: 101 3389 6158 1440 1453 3377 2131 1728 6889 1353 3681 1501 1314 2154 7344 1169 3454 891 3428 816 8024 3176 980 3389 704 5195 1072 924 782 1453 5001 5679 898 5637 4124 3173 1266 1765 3175 3791 7368 3596 2175 5392 3596 2175 2135 2900 2137 924 6349 7032 3173 5637 2395 126 5857 1039 8024 5445 2990 1139 4412 7032 1072 924 7026 3123 1762 3428 8024 3300 5637 4124 3173 1266 1765 3175 3791 7368 3596 2175 5392 1152 752 924 6349 7032 3119 3087 122 5158 1762 1318 1377 858 8020 6210 5637 4124 3173 1266 1765 3175 3791 7368 3596 2175 5392 8615 2399 2428 980 2099 5018 9833 8756 5998 980 3389 1318 5018 9674 7514 5520 7481 8021 8024 5760 1728 6158 1440 3176 3315 7368 8503 2399 2428 1333 6260 2099 5018 8164 5998 3681 1501 1314 2154 7344 1169 3454 891 3428 816 3976 991 4923 2415 3229 8024 5195 1394 3791 1001 1598 8024 699 6858 4761 1072 924 782 4719 914 6158 1440 1168 2431 2746 6244 8024 6158 1440 4192 3633 4534 4415 4507 5445 3313 6905 3309 1168 2431 8024 2541 5195 3315 7368 2872 2990 4192 5865 8039 1348 1072 924 782 5195 6858 4761 771 3313 6905 3309 6858 4761 2772 2380 1398 6158 1440 1168 3428 2970 1358 2746 6244 5023 2658 8024 3634 3300 3315 7368 6843 6888 6349 3292 124 819 510 2872 2990 1841 1440 510 1072 924 782 722 2786 2514 3124 6865 5178 868 3511 5143 5186 3389 6273 6536 3160 1392 122 819 7353 1318 1377 2731 8020 6210 3315 7368 1318 5018 8218 7514 510 5018 8164 5635 8245 7514 510 5018 8327 7514 510 5018 8247 7514 8021 511 1348 6158 1440 3313 1358 5398 2852 1350 1762 4675 1822 6121 8024 7549 2347 6845 1280 5023 2658 8024 2541 5195 3315 7368 898 5480 3609 3389 3209 2253 2179 8024 771 3300 5637 4124 7770 5023 3791 7368 6158 1440 1762 4675 1762 2852 1059 1751 5145 7087 6134 122 819 7353 1318 6639 2731 511 2986 6328 1184 2999 6303 3209 8024 5632 2746 2200 1072 924 782 1333 5260 5152 722 677 7274 924 6349 7032 1350 2179 3119 1164 2622 3760 1057 722 511 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/03/2020 17:58:13 - INFO - run_classifier -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/03/2020 17:58:13 - INFO - run_classifier -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/03/2020 17:58:13 - INFO - run_classifier -   label: 毒品危害防制 (id = 13)\n",
      "10/03/2020 17:58:13 - INFO - run_classifier -   *** Example ***\n",
      "10/03/2020 17:58:14 - INFO - run_classifier -   guid: train\n",
      "10/03/2020 17:58:14 - INFO - run_classifier -   tokens: [CLS] 於 100 年 10 月 16 日 20 時 30 分 許 ， 酒 後 無 照 駕 駛 車 牌 號 碼 「 07 ##10 - j ##f 」 號 自 小 客 車 ， 由 北 往 南 於 行 經 臺 南 市 ○○ 區 ○○ 路 與 永 平 路 口 時 ， 駕 車 衝 （ 追 ） 撞 同 方 向 由 陳 文 貴 所 騎 乘 之 腳 踏 車 車 尾 發 生 車 禍 ， 致 使 陳 文 貴 當 場 人 車 倒 地 並 因 而 受 有 「 頭 部 外 傷 、 臉 部 及 右 上 肢 及 雙 下 肢 多 處 挫 傷 」 等 傷 害 （ 過 失 傷 害 部 分 ， 未 據 告 訴 ） ； [UNK] 朱 興 昭 於 車 禍 肇 事 發 生 後 ， 竟 逕 自 駕 駛 肇 事 自 小 客 車 逃 離 現 場 ， 未 留 下 可 供 聯 絡 資 料 、 也 未 對 被 害 人 採 取 救 助 措 施 ， 經 路 人 駕 駛 機 車 自 後 追 隨 並 抄 下 肇 事 自 小 客 車 之 車 牌 號 碼 後 向 警 方 報 案 ， 陳 文 貴 則 經 路 人 攙 扶 至 路 邊 後 由 救 護 車 緊 急 送 醫 救 治 。 嗣 警 方 人 員 據 報 後 ， 循 線 於 同 日 20 時 50 分 許 ， 在 臺 南 市 永 康 區 ○○ ##○ 路 與 永 吉 路 口 ， 將 朱 興 昭 人 車 欄 停 後 ， 發 現 朱 興 昭 身 上 有 濃 厚 酒 味 ， 經 員 警 依 法 對 朱 興 昭 進 行 「 呼 氣 酒 精 濃 度 測 試 」 結 果 ， 朱 興 昭 之 呼 氣 酒 精 濃 度 測 試 值 為 每 公 升 o . 97 毫 克 [SEP]\n",
      "10/03/2020 17:58:14 - INFO - run_classifier -   input_ids: 101 3176 8135 2399 8108 3299 8121 3189 8113 3229 8114 1146 6258 8024 6983 2527 4192 4212 7690 7691 6722 4277 5998 4826 519 8155 8311 118 152 8189 520 5998 5632 2207 2145 6722 8024 4507 1266 2518 1298 3176 6121 5195 5637 1298 2356 9423 1281 9423 6662 5645 3719 2398 6662 1366 3229 8024 7690 6722 6128 8020 6841 8021 3058 1398 3175 1403 4507 7376 3152 6523 2792 7697 733 722 5589 6672 6722 6722 2227 4634 4495 6722 4884 8024 5636 886 7376 3152 6523 4534 1842 782 6722 948 1765 699 1728 5445 1358 3300 519 7531 6956 1912 1003 510 5622 6956 1350 1381 677 5501 1350 7427 678 5501 1914 5993 2919 1003 520 5023 1003 2154 8020 6882 1927 1003 2154 6956 1146 8024 3313 3087 1440 6260 8021 8039 100 3319 5646 3220 3176 6722 4884 5488 752 4634 4495 2527 8024 4994 6855 5632 7690 7691 5488 752 5632 2207 2145 6722 6845 7431 4412 1842 8024 3313 4522 678 1377 897 5474 5181 6536 3160 510 738 3313 2205 6158 2154 782 2967 1357 3131 1221 2974 3177 8024 5195 6662 782 7690 7691 3582 6722 5632 2527 6841 7401 699 2826 678 5488 752 5632 2207 2145 6722 722 6722 4277 5998 4826 2527 1403 6356 3175 1841 3428 8024 7376 3152 6523 1179 5195 6662 782 3107 2820 5635 6662 6920 2527 4507 3131 6362 6722 5215 2593 6843 7015 3131 3780 511 1632 6356 3175 782 1519 3087 1841 2527 8024 2542 5221 3176 1398 3189 8113 3229 8145 1146 6258 8024 1762 5637 1298 2356 3719 2434 1281 9423 10299 6662 5645 3719 1395 6662 1366 8024 2200 3319 5646 3220 782 6722 3608 977 2527 8024 4634 4412 3319 5646 3220 6716 677 3300 4083 1331 6983 1456 8024 5195 1519 6356 898 3791 2205 3319 5646 3220 6868 6121 519 1461 3706 6983 5125 4083 2428 3947 6275 520 5178 3362 8024 3319 5646 3220 722 1461 3706 6983 5125 4083 2428 3947 6275 966 4158 3680 1062 1285 157 119 8380 3690 1046 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/03/2020 17:58:14 - INFO - run_classifier -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/03/2020 17:58:14 - INFO - run_classifier -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/03/2020 17:58:14 - INFO - run_classifier -   label: 公共危險 (id = 2)\n",
      "10/03/2020 17:58:14 - INFO - run_classifier -   *** Example ***\n",
      "10/03/2020 17:58:14 - INFO - run_classifier -   guid: train\n",
      "10/03/2020 17:58:14 - INFO - run_classifier -   tokens: [CLS] 江 隆 健 （ 起 訴 書 誤 植 為 江 隆 建 ， 應 予 更 正 ） 前 與 彭 依 玲 為 男 女 朋 友 ， 並 共 同 居 住 在 彭 依 玲 位 於 在 臺 中 市 ○○ 區 ○○ 街 0 段 000 巷 0 ##○ ##00 號 住 處 ， [UNK] 其 竟 意 圖 為 自 己 不 法 之 所 有 ， 基 於 侵 占 之 犯 意 ， 自 民 國 105 年 3 月 底 某 日 起 至 106 年 7 月 29 日 止 ， 在 上 址 將 彭 依 玲 之 母 陳 嘉 瑛 所 購 買 交 與 彭 依 玲 使 用 之 車 牌 號 碼 000 - 000 ##0 號 普 通 重 型 機 車 騎 走 ， 並 據 為 己 用 ， 拒 不 返 還 。 [SEP]\n",
      "10/03/2020 17:58:14 - INFO - run_classifier -   input_ids: 101 3736 7384 978 8020 6629 6260 3292 6299 3490 4158 3736 7384 2456 8024 2746 750 3291 3633 8021 1184 5645 2510 898 4386 4158 4511 1957 3301 1351 8024 699 1066 1398 2233 857 1762 2510 898 4386 855 3176 1762 5637 704 2356 9423 1281 9423 6125 121 3667 8241 2350 121 10299 8279 5998 857 5993 8024 100 1071 4994 2692 1756 4158 5632 2346 679 3791 722 2792 3300 8024 1825 3176 909 1304 722 4306 2692 8024 5632 3696 1751 8423 2399 124 3299 2419 3378 3189 6629 5635 8438 2399 128 3299 8162 3189 3632 8024 1762 677 1770 2200 2510 898 4386 722 3678 7376 1649 4446 2792 6554 6525 769 5645 2510 898 4386 886 4500 722 6722 4277 5998 4826 8241 118 8241 8129 5998 3249 6858 7028 1798 3582 6722 7697 6624 8024 699 3087 4158 2346 4500 8024 2867 679 6819 6917 511 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/03/2020 17:58:14 - INFO - run_classifier -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/03/2020 17:58:14 - INFO - run_classifier -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/03/2020 17:58:14 - INFO - run_classifier -   label: 侵占 (id = 1)\n",
      "10/03/2020 17:58:14 - INFO - run_classifier -   *** Example ***\n",
      "10/03/2020 17:58:14 - INFO - run_classifier -   guid: train\n",
      "10/03/2020 17:58:14 - INFO - run_classifier -   tokens: [CLS] 緣 莊 秋 澤 之 友 人 李 定 台 前 委 由 許 融 富 （ 原 名 許 翼 權 ） 所 任 職 之 全 球 婚 友 社 辦 理 仲 介 迎 娶 越 南 新 娘 事 宜 ， 惟 未 能 順 利 娶 得 越 南 新 娘 來 臺 ， 遂 將 此 情 告 知 莊 秋 澤 ， 莊 秋 澤 即 撥 打 電 話 向 許 融 富 佯 稱 欲 委 請 其 介 紹 越 南 新 娘 ， 並 相 約 於 104 年 3 月 10 日 下 午 1 時 30 分 許 （ 起 訴 書 誤 載 為 「 下 午 1 時 40 分 許 」 ， 應 予 更 正 ） ， 在 臺 北 市 ○○ 區 ○○ ##○ 路 0 段 00 號 之 新 光 三 越 百 貨 前 見 面 ， 經 許 融 富 同 意 後 ， 莊 秋 澤 遂 邀 集 楊 世 忠 、 李 定 台 、 蔣 文 雄 （ 李 定 台 、 蔣 文 雄 所 涉 妨 害 自 由 部 分 ， 另 經 檢 察 官 為 不 起 訴 處 分 確 定 ） 一 同 前 往 。 莊 秋 澤 在 上 址 新 光 三 越 百 貨 前 見 許 融 富 後 ， 遂 以 臺 語 向 許 融 富 陳 稱 ： 「 我 萬 華 艋 押 的 ， 剛 從 監 獄 出 來 ， 李 定 台 花 了 幾 十 萬 元 ， 新 娘 卻 沒 有 來 臺 灣 ， 李 定 台 說 你 詐 欺 」 等 語 （ 非 屬 剝 奪 他 人 行 動 自 由 之 手 段 ） ， 並 出 手 拉 扯 許 融 富 ， 且 要 求 許 融 富 一 同 前 往 旁 邊 之 咖 啡 廳 協 商 ， 許 融 富 見 狀 向 後 閃 避 ， 莊 秋 澤 竟 與 楊 世 忠 共 同 基 於 剝 奪 他 人 行 動 自 由 之 犯 意 聯 絡 ， 先 由 莊 秋 澤 出 手 拉 扯 許 融 富 之 衣 服 以 阻 擋 其 離 去 ， 再 由 楊 世 忠 將 許 融 富 壓 制 在 地 ， 以 此 方 式 剝 奪 許 融 富 之 行 動 自 由 ， 嗣 經 楊 世 忠 報 警 處 理 ， 警 員 獲 報 後 於 同 日 下 午 1 時 39 分 抵 達 上 址 ， 許 融 富 始 回 復 自 由 。 [SEP]\n",
      "10/03/2020 17:58:14 - INFO - run_classifier -   input_ids: 101 5225 5800 4904 4075 722 1351 782 3330 2137 1378 1184 1999 4507 6258 6084 2168 8020 1333 1399 6258 5437 3609 8021 2792 818 5480 722 1059 4413 2042 1351 4852 6794 4415 815 792 6816 2034 6632 1298 3173 2023 752 2139 8024 2668 3313 5543 7518 1164 2034 2533 6632 1298 3173 2023 889 5637 8024 6876 2200 3634 2658 1440 4761 5800 4904 4075 8024 5800 4904 4075 1315 3060 2802 7442 6282 1403 6258 6084 2168 879 4935 3617 1999 6313 1071 792 5171 6632 1298 3173 2023 8024 699 4685 5147 3176 8503 2399 124 3299 8108 3189 678 1286 122 3229 8114 1146 6258 8020 6629 6260 3292 6299 6734 4158 519 678 1286 122 3229 8164 1146 6258 520 8024 2746 750 3291 3633 8021 8024 1762 5637 1266 2356 9423 1281 9423 10299 6662 121 3667 8136 5998 722 3173 1045 676 6632 4636 6515 1184 6210 7481 8024 5195 6258 6084 2168 1398 2692 2527 8024 5800 4904 4075 6876 6913 7415 3501 686 2566 510 3330 2137 1378 510 5919 3152 7413 8020 3330 2137 1378 510 5919 3152 7413 2792 3868 1981 2154 5632 4507 6956 1146 8024 1369 5195 3596 2175 2135 4158 679 6629 6260 5993 1146 4825 2137 8021 671 1398 1184 2518 511 5800 4904 4075 1762 677 1770 3173 1045 676 6632 4636 6515 1184 6210 6258 6084 2168 2527 8024 6876 809 5637 6295 1403 6258 6084 2168 7376 4935 8038 519 2769 5857 5836 5674 2852 4638 8024 1190 2537 4675 4352 1139 889 8024 3330 2137 1378 5709 749 2407 1282 5857 1039 8024 3173 2023 1320 3760 3300 889 5637 4124 8024 3330 2137 1378 6303 872 6266 3619 520 5023 6295 8020 7478 2253 1192 1954 800 782 6121 1240 5632 4507 722 2797 3667 8021 8024 699 1139 2797 2861 2816 6258 6084 2168 8024 684 6206 3724 6258 6084 2168 671 1398 1184 2518 3178 6920 722 1476 1565 2453 1295 1555 8024 6258 6084 2168 6210 4311 1403 2527 7272 6912 8024 5800 4904 4075 4994 5645 3501 686 2566 1066 1398 1825 3176 1192 1954 800 782 6121 1240 5632 4507 722 4306 2692 5474 5181 8024 1044 4507 5800 4904 4075 1139 2797 2861 2816 6258 6084 2168 722 6132 3302 809 7349 3081 1071 7431 1343 8024 1086 4507 3501 686 2566 2200 6258 6084 2168 1886 1169 1762 1765 8024 809 3634 3175 2466 1192 1954 6258 6084 2168 722 6121 1240 5632 4507 8024 1632 5195 3501 686 2566 1841 6356 5993 4415 8024 6356 1519 4363 1841 2527 3176 1398 3189 678 1286 122 3229 8240 1146 2850 6888 677 1770 8024 6258 6084 2168 1993 1726 2541 5632 4507 511 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/03/2020 17:58:14 - INFO - run_classifier -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/03/2020 17:58:14 - INFO - run_classifier -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/03/2020 17:58:14 - INFO - run_classifier -   label: 妨害自由 (id = 4)\n",
      "10/03/2020 17:58:14 - INFO - run_classifier -   *** Example ***\n",
      "10/03/2020 17:58:14 - INFO - run_classifier -   guid: train\n",
      "10/03/2020 17:58:14 - INFO - run_classifier -   tokens: [CLS] 被 告 游 世 傑 於 民 國 104 年 5 月 22 日 早 上 8 時 20 分 許 ， 騎 乘 車 牌 號 碼 000 - 000 號 普 通 重 型 機 車 ， 自 桃 園 市 ○○ 區 ○○ 路 000 號 社 區 地 下 4 樓 停 車 場 駛 至 停 車 場 1 樓 出 口 時 ， 本 應 注 意 車 輛 行 經 無 號 誌 之 交 岔 路 口 ， 應 減 速 慢 行 ， 作 隨 時 停 車 之 準 備 ， 且 當 時 天 候 陰 、 日 間 自 然 光 線 、 柏 油 路 面 乾 燥 、 無 缺 陷 、 無 障 礙 物 、 視 距 良 好 ， 並 無 不 能 注 意 之 情 事 ， 竟 疏 未 注 意 ， 未 減 速 慢 行 ， 即 貿 然 自 上 開 停 車 場 出 口 欲 左 轉 彎 進 入 大 有 路 ， 適 告 訴 人 張 誼 達 騎 乘 車 牌 號 碼 000 - 000 號 普 通 重 型 機 車 沿 大 有 路 ， 由 大 興 路 往 健 行 路 方 向 直 行 而 來 ， 見 狀 閃 避 不 及 ， 兩 車 因 而 發 生 碰 撞 ， 致 告 訴 人 張 誼 達 人 車 倒 地 ， 受 有 左 第 一 掌 腕 關 節 脫 臼 、 左 第 四 指 杵 狀 指 等 傷 害 。 [SEP]\n",
      "10/03/2020 17:58:14 - INFO - run_classifier -   input_ids: 101 6158 1440 3952 686 989 3176 3696 1751 8503 2399 126 3299 8130 3189 3193 677 129 3229 8113 1146 6258 8024 7697 733 6722 4277 5998 4826 8241 118 8241 5998 3249 6858 7028 1798 3582 6722 8024 5632 3425 1754 2356 9423 1281 9423 6662 8241 5998 4852 1281 1765 678 125 3559 977 6722 1842 7691 5635 977 6722 1842 122 3559 1139 1366 3229 8024 3315 2746 3800 2692 6722 6739 6121 5195 4192 5998 6290 722 769 2264 6662 1366 8024 2746 3938 6862 2714 6121 8024 868 7401 3229 977 6722 722 3976 991 8024 684 4534 3229 1921 952 7374 510 3189 7279 5632 4197 1045 5221 510 3377 3779 6662 7481 746 4246 510 4192 5375 7379 510 4192 7397 4844 4289 510 6213 6655 5679 1962 8024 699 4192 679 5543 3800 2692 722 2658 752 8024 4994 4541 3313 3800 2692 8024 3313 3938 6862 2714 6121 8024 1315 6530 4197 5632 677 7274 977 6722 1842 1139 1366 3617 2340 6752 2494 6868 1057 1920 3300 6662 8024 6900 1440 6260 782 2484 6309 6888 7697 733 6722 4277 5998 4826 8241 118 8241 5998 3249 6858 7028 1798 3582 6722 3784 1920 3300 6662 8024 4507 1920 5646 6662 2518 978 6121 6662 3175 1403 4684 6121 5445 889 8024 6210 4311 7272 6912 679 1350 8024 1060 6722 1728 5445 4634 4495 4821 3058 8024 5636 1440 6260 782 2484 6309 6888 782 6722 948 1765 8024 1358 3300 2340 5018 671 2958 5580 7302 5059 5562 5639 510 2340 5018 1724 2900 3348 4311 2900 5023 1003 2154 511 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/03/2020 17:58:14 - INFO - run_classifier -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/03/2020 17:58:14 - INFO - run_classifier -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/03/2020 17:58:14 - INFO - run_classifier -   label: 過失傷害 (id = 3)\n",
      "10/03/2020 17:58:15 - INFO - run_classifier -   ***** Running training *****\n",
      "10/03/2020 17:58:15 - INFO - run_classifier -     Num examples = 963\n",
      "10/03/2020 17:58:15 - INFO - run_classifier -     Batch size = 2\n",
      "10/03/2020 17:58:15 - INFO - run_classifier -     Num steps = 1443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's state_dict:\n",
      "bert.embeddings.word_embeddings.weight \t torch.Size([21128, 768])\n",
      "bert.embeddings.position_embeddings.weight \t torch.Size([512, 768])\n",
      "bert.embeddings.token_type_embeddings.weight \t torch.Size([2, 768])\n",
      "bert.embeddings.LayerNorm.weight \t torch.Size([768])\n",
      "bert.embeddings.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.0.attention.self.query.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.self.query.bias \t torch.Size([768])\n",
      "bert.encoder.layer.0.attention.self.key.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.self.key.bias \t torch.Size([768])\n",
      "bert.encoder.layer.0.attention.self.value.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.self.value.bias \t torch.Size([768])\n",
      "bert.encoder.layer.0.attention.output.dense.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.0.intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "bert.encoder.layer.0.intermediate.dense.bias \t torch.Size([3072])\n",
      "bert.encoder.layer.0.output.dense.weight \t torch.Size([768, 3072])\n",
      "bert.encoder.layer.0.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.0.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.0.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.1.attention.self.query.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.self.query.bias \t torch.Size([768])\n",
      "bert.encoder.layer.1.attention.self.key.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.self.key.bias \t torch.Size([768])\n",
      "bert.encoder.layer.1.attention.self.value.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.self.value.bias \t torch.Size([768])\n",
      "bert.encoder.layer.1.attention.output.dense.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.1.intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "bert.encoder.layer.1.intermediate.dense.bias \t torch.Size([3072])\n",
      "bert.encoder.layer.1.output.dense.weight \t torch.Size([768, 3072])\n",
      "bert.encoder.layer.1.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.1.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.1.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.2.attention.self.query.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.self.query.bias \t torch.Size([768])\n",
      "bert.encoder.layer.2.attention.self.key.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.self.key.bias \t torch.Size([768])\n",
      "bert.encoder.layer.2.attention.self.value.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.self.value.bias \t torch.Size([768])\n",
      "bert.encoder.layer.2.attention.output.dense.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.2.intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "bert.encoder.layer.2.intermediate.dense.bias \t torch.Size([3072])\n",
      "bert.encoder.layer.2.output.dense.weight \t torch.Size([768, 3072])\n",
      "bert.encoder.layer.2.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.2.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.2.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.3.attention.self.query.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.self.query.bias \t torch.Size([768])\n",
      "bert.encoder.layer.3.attention.self.key.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.self.key.bias \t torch.Size([768])\n",
      "bert.encoder.layer.3.attention.self.value.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.self.value.bias \t torch.Size([768])\n",
      "bert.encoder.layer.3.attention.output.dense.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.3.intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "bert.encoder.layer.3.intermediate.dense.bias \t torch.Size([3072])\n",
      "bert.encoder.layer.3.output.dense.weight \t torch.Size([768, 3072])\n",
      "bert.encoder.layer.3.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.3.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.3.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.4.attention.self.query.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.self.query.bias \t torch.Size([768])\n",
      "bert.encoder.layer.4.attention.self.key.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.self.key.bias \t torch.Size([768])\n",
      "bert.encoder.layer.4.attention.self.value.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.self.value.bias \t torch.Size([768])\n",
      "bert.encoder.layer.4.attention.output.dense.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.4.intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "bert.encoder.layer.4.intermediate.dense.bias \t torch.Size([3072])\n",
      "bert.encoder.layer.4.output.dense.weight \t torch.Size([768, 3072])\n",
      "bert.encoder.layer.4.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.4.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.4.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.5.attention.self.query.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.self.query.bias \t torch.Size([768])\n",
      "bert.encoder.layer.5.attention.self.key.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.self.key.bias \t torch.Size([768])\n",
      "bert.encoder.layer.5.attention.self.value.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.self.value.bias \t torch.Size([768])\n",
      "bert.encoder.layer.5.attention.output.dense.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.5.intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "bert.encoder.layer.5.intermediate.dense.bias \t torch.Size([3072])\n",
      "bert.encoder.layer.5.output.dense.weight \t torch.Size([768, 3072])\n",
      "bert.encoder.layer.5.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.5.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.5.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.6.attention.self.query.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.self.query.bias \t torch.Size([768])\n",
      "bert.encoder.layer.6.attention.self.key.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.self.key.bias \t torch.Size([768])\n",
      "bert.encoder.layer.6.attention.self.value.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.self.value.bias \t torch.Size([768])\n",
      "bert.encoder.layer.6.attention.output.dense.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.6.intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "bert.encoder.layer.6.intermediate.dense.bias \t torch.Size([3072])\n",
      "bert.encoder.layer.6.output.dense.weight \t torch.Size([768, 3072])\n",
      "bert.encoder.layer.6.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.6.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.6.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.7.attention.self.query.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.self.query.bias \t torch.Size([768])\n",
      "bert.encoder.layer.7.attention.self.key.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.self.key.bias \t torch.Size([768])\n",
      "bert.encoder.layer.7.attention.self.value.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.self.value.bias \t torch.Size([768])\n",
      "bert.encoder.layer.7.attention.output.dense.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.7.intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "bert.encoder.layer.7.intermediate.dense.bias \t torch.Size([3072])\n",
      "bert.encoder.layer.7.output.dense.weight \t torch.Size([768, 3072])\n",
      "bert.encoder.layer.7.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.7.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.7.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.8.attention.self.query.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.self.query.bias \t torch.Size([768])\n",
      "bert.encoder.layer.8.attention.self.key.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.self.key.bias \t torch.Size([768])\n",
      "bert.encoder.layer.8.attention.self.value.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.self.value.bias \t torch.Size([768])\n",
      "bert.encoder.layer.8.attention.output.dense.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.8.intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "bert.encoder.layer.8.intermediate.dense.bias \t torch.Size([3072])\n",
      "bert.encoder.layer.8.output.dense.weight \t torch.Size([768, 3072])\n",
      "bert.encoder.layer.8.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.8.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.8.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.9.attention.self.query.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.self.query.bias \t torch.Size([768])\n",
      "bert.encoder.layer.9.attention.self.key.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.self.key.bias \t torch.Size([768])\n",
      "bert.encoder.layer.9.attention.self.value.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.self.value.bias \t torch.Size([768])\n",
      "bert.encoder.layer.9.attention.output.dense.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.9.intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "bert.encoder.layer.9.intermediate.dense.bias \t torch.Size([3072])\n",
      "bert.encoder.layer.9.output.dense.weight \t torch.Size([768, 3072])\n",
      "bert.encoder.layer.9.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.9.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.9.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.10.attention.self.query.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.self.query.bias \t torch.Size([768])\n",
      "bert.encoder.layer.10.attention.self.key.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.self.key.bias \t torch.Size([768])\n",
      "bert.encoder.layer.10.attention.self.value.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.self.value.bias \t torch.Size([768])\n",
      "bert.encoder.layer.10.attention.output.dense.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.10.intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "bert.encoder.layer.10.intermediate.dense.bias \t torch.Size([3072])\n",
      "bert.encoder.layer.10.output.dense.weight \t torch.Size([768, 3072])\n",
      "bert.encoder.layer.10.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.10.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.10.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.11.attention.self.query.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.self.query.bias \t torch.Size([768])\n",
      "bert.encoder.layer.11.attention.self.key.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.self.key.bias \t torch.Size([768])\n",
      "bert.encoder.layer.11.attention.self.value.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.self.value.bias \t torch.Size([768])\n",
      "bert.encoder.layer.11.attention.output.dense.weight \t torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.encoder.layer.11.intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "bert.encoder.layer.11.intermediate.dense.bias \t torch.Size([3072])\n",
      "bert.encoder.layer.11.output.dense.weight \t torch.Size([768, 3072])\n",
      "bert.encoder.layer.11.output.dense.bias \t torch.Size([768])\n",
      "bert.encoder.layer.11.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert.encoder.layer.11.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert.pooler.dense.weight \t torch.Size([768, 768])\n",
      "bert.pooler.dense.bias \t torch.Size([768])\n",
      "classifier.weight \t torch.Size([18, 768])\n",
      "classifier.bias \t torch.Size([18])\n",
      "Optimizer's state_dict:\n",
      "state \t {}\n",
      "param_groups \t [{'weight_decay': 0.01, 'lr': 5e-05, 'schedule': <pytorch_pretrained_bert.optimization.WarmupLinearSchedule object at 0x00000287325C8CC0>, 'b1': 0.9, 'b2': 0.999, 'e': 1e-06, 'max_grad_norm': 1.0, 'params': [2779709557136, 2779709557568, 2779670587144, 2779709558648, 2779709524224, 2779709648112, 2779709680088, 2779712697184, 2779712698768, 2779713774432, 2779713774576, 2779713774720, 2779713774864, 2779713775584, 2779713776016, 2779713776808, 2779713776952, 2779713777096, 2779713777240, 2779714679144, 2779714679576, 2779714680368, 2779714680512, 2779714680656, 2779714680800, 2779714681520, 2779714681952, 2779714682744, 2779714768968, 2779714769112, 2779714769256, 2779714769976, 2779714770408, 2779714771200, 2779714771344, 2779714771488, 2779714771632, 2779714772352, 2779714772784, 2779691754120, 2779691754264, 2779691754408, 2779691754552, 2779691755272, 2779691755704, 2779691756352, 2779691756496, 2779691756640, 2779691756784, 2779691757504, 2779689185712, 2779689186504, 2779689186648, 2779689186792, 2779689186936, 2779689187656, 2779689188088, 2779689188880, 2779689189024, 2779689189168, 2779689189312, 2779689071312, 2779689071744, 2779689072536, 2779689072680, 2779689072824, 2779689072968, 2779689073688, 2779689074120, 2779688988960, 2779688989104, 2779688989248, 2779688989392, 2779688990112, 2779688990544, 2779713774072, 2779713773928]}, {'weight_decay': 0.0, 'lr': 5e-05, 'schedule': <pytorch_pretrained_bert.optimization.WarmupLinearSchedule object at 0x00000287325C8CC0>, 'b1': 0.9, 'b2': 0.999, 'e': 1e-06, 'max_grad_norm': 1.0, 'params': [2779670587072, 2779670589088, 2779709522424, 2779709525016, 2779709648184, 2779709680160, 2779709680952, 2779709681024, 2779712697256, 2779712698840, 2779712699632, 2779712699704, 2779713774504, 2779713774648, 2779713774792, 2779713774936, 2779713775008, 2779713775080, 2779713775656, 2779713776088, 2779713776304, 2779713776376, 2779713776880, 2779713777024, 2779713777168, 2779713777312, 2779713777384, 2779713777456, 2779714679216, 2779714679648, 2779714679864, 2779714679936, 2779714680440, 2779714680584, 2779714680728, 2779714680872, 2779714680944, 2779714681016, 2779714681592, 2779714682024, 2779714682240, 2779714682312, 2779714682816, 2779714769040, 2779714769184, 2779714769328, 2779714769400, 2779714769472, 2779714770048, 2779714770480, 2779714770696, 2779714770768, 2779714771272, 2779714771416, 2779714771560, 2779714771704, 2779714771776, 2779714771848, 2779714772424, 2779714772856, 2779691753616, 2779691753688, 2779691754192, 2779691754336, 2779691754480, 2779691754624, 2779691754696, 2779691754768, 2779691755344, 2779691755776, 2779691755848, 2779691755920, 2779691756424, 2779691756568, 2779691756712, 2779691756856, 2779691756928, 2779691757000, 2779689185352, 2779689185784, 2779689186000, 2779689186072, 2779689186576, 2779689186720, 2779689186864, 2779689187008, 2779689187080, 2779689187152, 2779689187728, 2779689188160, 2779689188376, 2779689188448, 2779689188952, 2779689189096, 2779689189240, 2779689070664, 2779689070736, 2779689070808, 2779689071384, 2779689071816, 2779689072032, 2779689072104, 2779689072608, 2779689072752, 2779689072896, 2779689073040, 2779689073112, 2779689073184, 2779689073760, 2779689074192, 2779689074408, 2779689074480, 2779688989032, 2779688989176, 2779688989320, 2779688989464, 2779688989536, 2779688989608, 2779688990184, 2779688990616, 2779688990832, 2779688990904, 2779713774000, 2779713773856]}]\n"
     ]
    },
    {
     "data": {
      "text/plain": "'\\ntorch.save(model.state_dict(), \\'BertModel\\')\\n\\ntorch.cuda.empty_cache()\\n\\nmodel = torch.load(\"BertModel\", map_location=torch.device(\\'cpu\\'))\\n'"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "QtBQqVbDuFxk",
    "colab_type": "code",
    "outputId": "6a9a6963-458b-453e-9740-7e22034181a2",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1577895539789,
     "user_tz": -480,
     "elapsed": 1396805,
     "user": {
      "displayName": "曹錫璋",
      "photoUrl": "",
      "userId": "12089102222035734305"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 595
    }
   },
   "source": [
    "for _ in trange(int(num_train_epochs), desc=\"Epoch\"):\n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    total_step = len(train_data) // train_batch_size\n",
    "    ten_percent_step = total_step // 10\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        input_ids, input_mask, segment_ids, label_ids = batch\n",
    "        loss = model(input_ids, segment_ids, input_mask, label_ids)\n",
    "        if n_gpu > 1:\n",
    "            loss = loss.mean() # mean() to average on multi-gpu.\n",
    "        if gradient_accumulation_steps > 1:\n",
    "            loss = loss / gradient_accumulation_steps\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "        if (step + 1) % gradient_accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            global_step += 1\n",
    "            \n",
    "        if step % ten_percent_step == 0:\n",
    "            print(\"Fininshed: {:.2f}% ({}/{})\".format(step/total_step*100, step, total_step))"
   ],
   "execution_count": 18,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  67%|██████▋   | 2/3 [05:31<02:43, 163.91s/it]10/03/2020 18:06:38 - WARNING - pytorch_pretrained_bert.optimization -   Training beyond specified 't_total'. Learning rate multiplier set to 0.0. Please set 't_total' of WarmupLinearSchedule correctly.\n",
      "10/03/2020 18:06:39 - WARNING - pytorch_pretrained_bert.optimization -   Training beyond specified 't_total'. Learning rate multiplier set to 0.0. Please set 't_total' of WarmupLinearSchedule correctly.\n",
      "Epoch: 100%|██████████| 3/3 [08:22<00:00, 167.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fininshed: 0.00% (0/481)\n",
      "Fininshed: 9.98% (48/481)\n",
      "Fininshed: 19.96% (96/481)\n",
      "Fininshed: 29.94% (144/481)\n",
      "Fininshed: 39.92% (192/481)\n",
      "Fininshed: 49.90% (240/481)\n",
      "Fininshed: 59.88% (288/481)\n",
      "Fininshed: 69.85% (336/481)\n",
      "Fininshed: 79.83% (384/481)\n",
      "Fininshed: 89.81% (432/481)\n",
      "Fininshed: 99.79% (480/481)\n",
      "Fininshed: 0.00% (0/481)\n",
      "Fininshed: 9.98% (48/481)\n",
      "Fininshed: 19.96% (96/481)\n",
      "Fininshed: 29.94% (144/481)\n",
      "Fininshed: 39.92% (192/481)\n",
      "Fininshed: 49.90% (240/481)\n",
      "Fininshed: 59.88% (288/481)\n",
      "Fininshed: 69.85% (336/481)\n",
      "Fininshed: 79.83% (384/481)\n",
      "Fininshed: 89.81% (432/481)\n",
      "Fininshed: 99.79% (480/481)\n",
      "Fininshed: 0.00% (0/481)\n",
      "Fininshed: 9.98% (48/481)\n",
      "Fininshed: 19.96% (96/481)\n",
      "Fininshed: 29.94% (144/481)\n",
      "Fininshed: 39.92% (192/481)\n",
      "Fininshed: 49.90% (240/481)\n",
      "Fininshed: 59.88% (288/481)\n",
      "Fininshed: 69.85% (336/481)\n",
      "Fininshed: 79.83% (384/481)\n",
      "Fininshed: 89.81% (432/481)\n",
      "Fininshed: 99.79% (480/481)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "_uuid": "bb25f1d8d6226c8d4bfe22ed1efcee1be368e7ff",
    "id": "D5gd0G3TuFxq",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Save a trained model and the associated configuration\n",
    "model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n",
    "output_model_file = os.path.join(output_dir, WEIGHTS_NAME)\n",
    "torch.save(model_to_save.state_dict(), output_model_file)\n",
    "output_config_file = os.path.join(output_dir, CONFIG_NAME)\n",
    "with open(output_config_file, 'w') as f:\n",
    "    f.write(model_to_save.config.to_json_string())"
   ],
   "execution_count": 19,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "_uuid": "aa7c59dd0eacdd89c629e97d91be718f70f7c5dc",
    "id": "fjvrZ9QIuFxx",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# Load a trained model and config that you have fine-tuned\n",
    "output_config_file = os.path.join(output_dir, CONFIG_NAME)\n",
    "output_model_file = os.path.join(output_dir, WEIGHTS_NAME)\n",
    "config = BertConfig(output_config_file)\n",
    "model = BertForSequenceClassification(config, num_labels=len(label_list))\n",
    "model.load_state_dict(torch.load(output_model_file))\n",
    "model.to(device)  # important to specific device\n",
    "if n_gpu > 1:\n",
    "    model = torch.nn.DataParallel(model)"
   ],
   "execution_count": 20,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "_uuid": "35c1906644d8c1d1789bc105884142e967bcfd87",
    "id": "NvXK2ertuFx7",
    "colab_type": "code",
    "outputId": "678d1a55-8a04-43d9-954d-3b04d83ef162",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1577895544153,
     "user_tz": -480,
     "elapsed": 61,
     "user": {
      "displayName": "曹錫璋",
      "photoUrl": "",
      "userId": "12089102222035734305"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    }
   },
   "source": [
    "config"
   ],
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "{\n  \"attention_probs_dropout_prob\": 0.1,\n  \"directionality\": \"bidi\",\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"max_position_embeddings\": 512,\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pooler_fc_size\": 768,\n  \"pooler_num_attention_heads\": 12,\n  \"pooler_num_fc_layers\": 3,\n  \"pooler_size_per_head\": 128,\n  \"pooler_type\": \"first_token_transform\",\n  \"type_vocab_size\": 2,\n  \"vocab_size\": 21128\n}"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "_uuid": "5fcfd222826070652e34c494502f1b7f3288d180",
    "id": "f54VoHdsuFyO",
    "colab_type": "code",
    "outputId": "f4ad1828-27f8-4652-b4ea-2bce2935eebf",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1577895622783,
     "user_tz": -480,
     "elapsed": 78637,
     "user": {
      "displayName": "曹錫璋",
      "photoUrl": "",
      "userId": "12089102222035734305"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 768
    }
   },
   "source": [
    "# val\n",
    "eval_examples = val_examples\n",
    "eval_features = convert_examples_to_features(\n",
    "    eval_examples, label_list, max_seq_length, tokenizer)\n",
    "logger.info(\"***** Running evaluation *****\")\n",
    "logger.info(\"  Num examples = %d\", len(eval_examples))\n",
    "logger.info(\"  Batch size = %d\", eval_batch_size)\n",
    "all_input_ids = torch.tensor([f.input_ids for f in eval_features], dtype=torch.long)\n",
    "all_input_mask = torch.tensor([f.input_mask for f in eval_features], dtype=torch.long)\n",
    "all_segment_ids = torch.tensor([f.segment_ids for f in eval_features], dtype=torch.long)\n",
    "all_label_ids = torch.tensor([f.label_id for f in eval_features], dtype=torch.long)\n",
    "eval_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "# Run prediction for full data\n",
    "eval_sampler = SequentialSampler(eval_data)\n",
    "eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=eval_batch_size)\n",
    "\n",
    "model.eval()\n",
    "eval_loss, eval_accuracy = 0, 0\n",
    "nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "for input_ids, input_mask, segment_ids, label_ids in eval_dataloader:\n",
    "    input_ids = input_ids.to(device)\n",
    "    input_mask = input_mask.to(device)\n",
    "    segment_ids = segment_ids.to(device)\n",
    "    label_ids = label_ids.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        tmp_eval_loss = model(input_ids, segment_ids, input_mask, label_ids)\n",
    "        logits = model(input_ids, segment_ids, input_mask)\n",
    "\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = label_ids.to('cpu').numpy()\n",
    "    tmp_eval_accuracy = accuracy(logits, label_ids)\n",
    "\n",
    "    eval_loss += tmp_eval_loss.mean().item()\n",
    "    eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "    nb_eval_examples += input_ids.size(0)\n",
    "    nb_eval_steps += 1\n",
    "\n",
    "eval_loss = eval_loss / nb_eval_steps\n",
    "eval_accuracy = eval_accuracy / nb_eval_examples\n",
    "loss = tr_loss/nb_tr_steps\n",
    "result = {'eval_loss': eval_loss,\n",
    "          'eval_accuracy': eval_accuracy,\n",
    "          'global_step': global_step,\n",
    "          'loss': loss}\n",
    "\n",
    "output_eval_file = os.path.join(output_dir, \"eval_results.txt\")\n",
    "with open(output_eval_file, \"w\") as writer:\n",
    "    logger.info(\"***** Eval results *****\")\n",
    "    for key in sorted(result.keys()):\n",
    "        logger.info(\"  %s = %s\", key, str(result[key]))\n",
    "        writer.write(\"%s = %s\\n\" % (key, str(result[key])))"
   ],
   "execution_count": 22,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/03/2020 18:06:42 - INFO - run_classifier -   *** Example ***\n",
      "10/03/2020 18:06:42 - INFO - run_classifier -   guid: val\n",
      "10/03/2020 18:06:42 - INFO - run_classifier -   tokens: [CLS] 被 告 陳 盈 貝 於 民 國 104 年 2 月 24 日 晚 間 8 時 16 分 許 ， 駕 駛 車 牌 號 碼 000 ##0 - 00 號 之 自 用 小 客 車 ， 沿 桃 園 市 桃 園 區 桃 鶯 路 往 桃 園 區 方 向 行 駛 ， 行 經 桃 園 市 ○○ 區 ○○ 路 000 號 前 ， 本 應 注 意 後 方 車 超 越 前 行 車 時 ， 應 保 持 半 公 尺 以 上 之 間 隔 超 過 ， 且 依 當 時 情 況 並 無 不 能 注 意 之 情 事 ， 竟 疏 未 注 意 ， 於 未 保 持 安 全 距 離 之 情 況 下 ， 超 越 前 方 告 訴 人 李 金 玉 所 騎 乘 車 牌 號 碼 000 - 000 號 之 普 通 重 型 機 車 ， 兩 車 遂 因 此 發 生 碰 撞 ， 致 告 訴 人 因 此 受 有 頸 椎 外 傷 併 椎 間 盤 突 出 、 脊 髓 病 變 之 傷 害 。 [SEP]\n",
      "10/03/2020 18:06:42 - INFO - run_classifier -   input_ids: 101 6158 1440 7376 4659 6509 3176 3696 1751 8503 2399 123 3299 8125 3189 3241 7279 129 3229 8121 1146 6258 8024 7690 7691 6722 4277 5998 4826 8241 8129 118 8136 5998 722 5632 4500 2207 2145 6722 8024 3784 3425 1754 2356 3425 1754 1281 3425 7873 6662 2518 3425 1754 1281 3175 1403 6121 7691 8024 6121 5195 3425 1754 2356 9423 1281 9423 6662 8241 5998 1184 8024 3315 2746 3800 2692 2527 3175 6722 6631 6632 1184 6121 6722 3229 8024 2746 924 2898 1288 1062 2223 809 677 722 7279 7392 6631 6882 8024 684 898 4534 3229 2658 3785 699 4192 679 5543 3800 2692 722 2658 752 8024 4994 4541 3313 3800 2692 8024 3176 3313 924 2898 2128 1059 6655 7431 722 2658 3785 678 8024 6631 6632 1184 3175 1440 6260 782 3330 7032 4373 2792 7697 733 6722 4277 5998 4826 8241 118 8241 5998 722 3249 6858 7028 1798 3582 6722 8024 1060 6722 6876 1728 3634 4634 4495 4821 3058 8024 5636 1440 6260 782 1728 3634 1358 3300 7534 3491 1912 1003 882 3491 7279 4676 4960 1139 510 5550 7767 4567 6365 722 1003 2154 511 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/03/2020 18:06:42 - INFO - run_classifier -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/03/2020 18:06:42 - INFO - run_classifier -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/03/2020 18:06:42 - INFO - run_classifier -   label: 過失傷害 (id = 3)\n",
      "10/03/2020 18:06:42 - INFO - run_classifier -   *** Example ***\n",
      "10/03/2020 18:06:42 - INFO - run_classifier -   guid: val\n",
      "10/03/2020 18:06:42 - INFO - run_classifier -   tokens: [CLS] 被 告 潘 秀 美 於 民 國 104 年 9 月 2 日 下 午 某 時 許 ， 駕 駛 車 牌 號 碼 000 ##0 - 00 號 自 用 小 客 車 ， 沿 桃 園 市 中 壢 區 龍 福 路 往 萬 坪 公 園 方 向 行 駛 ， [UNK] 於 同 日 13 時 59 分 許 ， 行 經 中 壢 區 龍 福 與 龍 東 十 二 街 之 交 岔 口 時 ， 本 應 注 意 行 經 無 號 誌 之 交 岔 路 口 時 ， 應 減 速 慢 行 ， 作 隨 時 停 車 之 準 備 ， 及 應 注 意 車 前 狀 況 、 採 取 必 要 之 安 全 措 施 ， 而 當 時 天 氣 晴 、 無 障 礙 物 、 日 間 自 然 光 線 、 視 距 良 好 、 柏 油 路 面 ， 並 無 不 能 注 意 之 情 事 ， 竟 疏 未 注 意 及 此 ， 貿 然 行 駛 欲 穿 越 上 開 交 岔 路 口 直 行 ， 適 有 告 訴 人 何 玉 英 騎 乘 車 牌 號 碼 000 - 000 ##0 號 普 通 重 型 機 車 直 行 中 壢 區 龍 東 十 二 街 騎 駛 至 此 ， 因 閃 避 不 及 而 發 生 碰 撞 ， 致 人 車 倒 地 ， 何 玉 英 因 而 受 有 右 鎖 骨 閉 鎖 性 骨 折 、 右 踝 閉 鎖 性 骨 折 及 距 腓 前 韌 帶 斷 裂 等 傷 害 ， [SEP]\n",
      "10/03/2020 18:06:42 - INFO - run_classifier -   input_ids: 101 6158 1440 4050 4899 5401 3176 3696 1751 8503 2399 130 3299 123 3189 678 1286 3378 3229 6258 8024 7690 7691 6722 4277 5998 4826 8241 8129 118 8136 5998 5632 4500 2207 2145 6722 8024 3784 3425 1754 2356 704 1891 1281 7983 4886 6662 2518 5857 1790 1062 1754 3175 1403 6121 7691 8024 100 3176 1398 3189 8124 3229 8257 1146 6258 8024 6121 5195 704 1891 1281 7983 4886 5645 7983 3346 1282 753 6125 722 769 2264 1366 3229 8024 3315 2746 3800 2692 6121 5195 4192 5998 6290 722 769 2264 6662 1366 3229 8024 2746 3938 6862 2714 6121 8024 868 7401 3229 977 6722 722 3976 991 8024 1350 2746 3800 2692 6722 1184 4311 3785 510 2967 1357 2553 6206 722 2128 1059 2974 3177 8024 5445 4534 3229 1921 3706 3252 510 4192 7397 4844 4289 510 3189 7279 5632 4197 1045 5221 510 6213 6655 5679 1962 510 3377 3779 6662 7481 8024 699 4192 679 5543 3800 2692 722 2658 752 8024 4994 4541 3313 3800 2692 1350 3634 8024 6530 4197 6121 7691 3617 4959 6632 677 7274 769 2264 6662 1366 4684 6121 8024 6900 3300 1440 6260 782 862 4373 5739 7697 733 6722 4277 5998 4826 8241 118 8241 8129 5998 3249 6858 7028 1798 3582 6722 4684 6121 704 1891 1281 7983 3346 1282 753 6125 7697 7691 5635 3634 8024 1728 7272 6912 679 1350 5445 4634 4495 4821 3058 8024 5636 782 6722 948 1765 8024 862 4373 5739 1728 5445 1358 3300 1381 7115 7755 7273 7115 2595 7755 2835 510 1381 6674 7273 7115 2595 7755 2835 1350 6655 5578 1184 7501 2380 3174 6162 5023 1003 2154 8024 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/03/2020 18:06:42 - INFO - run_classifier -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/03/2020 18:06:42 - INFO - run_classifier -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/03/2020 18:06:42 - INFO - run_classifier -   label: 過失傷害 (id = 3)\n",
      "10/03/2020 18:06:42 - INFO - run_classifier -   *** Example ***\n",
      "10/03/2020 18:06:42 - INFO - run_classifier -   guid: val\n",
      "10/03/2020 18:06:42 - INFO - run_classifier -   tokens: [CLS] 王 世 昌 前 因 施 用 毒 品 案 件 ， 經 依 臺 灣 板 橋 地 方 法 院 （ 現 更 名 為 臺 灣 新 北 地 方 法 院 ， 下 同 ） 裁 定 送 觀 察 、 勒 戒 後 ， 因 認 無 繼 續 施 用 毒 品 之 傾 向 ， 於 民 國 94 年 7 月 7 日 釋 放 出 所 ， 並 由 本 署 檢 察 官 以 94 年 度 毒 偵 字 第 299 ##9 號 為 不 起 訴 處 分 確 定 ； 又 因 施 用 毒 品 案 件 ， 經 同 法 院 以 98 年 度 訴 字 第 140 ##9 號 判 決 判 處 有 期 徒 刑 7 月 確 定 ， 於 99 年 2 月 22 日 執 行 完 畢 出 監 ； 再 因 施 用 毒 品 案 件 ， 經 臺 灣 新 北 地 方 法 院 以 104 年 度 審 訴 字 第 57 ##4 號 判 決 判 處 有 期 徒 刑 8 月 ， 嗣 經 臺 灣 高 等 法 院 以 104 年 度 上 訴 字 第 1935 號 駁 回 上 訴 確 定 ， 於 105 年 6 月 22 日 執 行 完 畢 出 監 。 [UNK] 仍 不 知 悔 改 ， 復 基 於 施 用 第 一 級 毒 品 之 犯 意 ， 於 105 年 11 月 15 日 23 時 48 分 為 警 採 尿 時 起 回 溯 26 小 時 內 之 某 時 許 ， 在 臺 灣 地 區 不 詳 地 點 ， 以 不 詳 方 式 ， 施 用 第 一 級 毒 品 海 洛 因 1 次 ， 嗣 於 105 年 11 月 15 日 23 時 40 分 許 ， 在 新 北 市 三 重 區 自 強 路 1 段 與 文 化 北 路 口 ， 因 其 為 毒 品 列 管 人 口 為 警 查 獲 ， 經 警 採 集 其 尿 液 送 驗 後 ， 結 果 呈 可 待 因 、 嗎 啡 陽 性 反 應 。 [SEP]\n",
      "10/03/2020 18:06:42 - INFO - run_classifier -   input_ids: 101 4374 686 3208 1184 1728 3177 4500 3681 1501 3428 816 8024 5195 898 5637 4124 3352 3578 1765 3175 3791 7368 8020 4412 3291 1399 4158 5637 4124 3173 1266 1765 3175 3791 7368 8024 678 1398 8021 6161 2137 6843 6223 2175 510 1239 2770 2527 8024 1728 6291 4192 5262 5265 3177 4500 3681 1501 722 1005 1403 8024 3176 3696 1751 8416 2399 128 3299 128 3189 7026 3123 1139 2792 8024 699 4507 3315 5392 3596 2175 2135 809 8416 2399 2428 3681 980 2099 5018 9600 8160 5998 4158 679 6629 6260 5993 1146 4825 2137 8039 1348 1728 3177 4500 3681 1501 3428 816 8024 5195 1398 3791 7368 809 8327 2399 2428 6260 2099 5018 8468 8160 5998 1161 3748 1161 5993 3300 3309 2530 1152 128 3299 4825 2137 8024 3176 8238 2399 123 3299 8130 3189 1822 6121 2130 4525 1139 4675 8039 1086 1728 3177 4500 3681 1501 3428 816 8024 5195 5637 4124 3173 1266 1765 3175 3791 7368 809 8503 2399 2428 2182 6260 2099 5018 8272 8159 5998 1161 3748 1161 5993 3300 3309 2530 1152 129 3299 8024 1632 5195 5637 4124 7770 5023 3791 7368 809 8503 2399 2428 677 6260 2099 5018 9523 5998 7684 1726 677 6260 4825 2137 8024 3176 8423 2399 127 3299 8130 3189 1822 6121 2130 4525 1139 4675 511 100 793 679 4761 2637 3121 8024 2541 1825 3176 3177 4500 5018 671 5159 3681 1501 722 4306 2692 8024 3176 8423 2399 8111 3299 8115 3189 8133 3229 8214 1146 4158 6356 2967 2228 3229 6629 1726 3985 8153 2207 3229 1058 722 3378 3229 6258 8024 1762 5637 4124 1765 1281 679 6284 1765 7953 8024 809 679 6284 3175 2466 8024 3177 4500 5018 671 5159 3681 1501 3862 3821 1728 122 3613 8024 1632 3176 8423 2399 8111 3299 8115 3189 8133 3229 8164 1146 6258 8024 1762 3173 1266 2356 676 7028 1281 5632 2485 6662 122 3667 5645 3152 1265 1266 6662 1366 8024 1728 1071 4158 3681 1501 1154 5052 782 1366 4158 6356 3389 4363 8024 5195 6356 2967 7415 1071 2228 3890 6843 7710 2527 8024 5178 3362 1439 1377 2521 1728 510 1621 1565 7382 2595 1353 2746 511 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/03/2020 18:06:42 - INFO - run_classifier -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/03/2020 18:06:42 - INFO - run_classifier -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/03/2020 18:06:42 - INFO - run_classifier -   label: 毒品危害防制 (id = 13)\n",
      "10/03/2020 18:06:42 - INFO - run_classifier -   *** Example ***\n",
      "10/03/2020 18:06:42 - INFO - run_classifier -   guid: val\n",
      "10/03/2020 18:06:42 - INFO - run_classifier -   tokens: [CLS] 趙 平 義 於 民 國 100 年 7 月 13 日 下 午 4 時 20 分 許 ， 駕 駛 車 牌 號 碼 j ##4 - 349 ##7 號 自 小 貨 車 ， 沿 臺 南 市 ○ 區 ○○ 街 由 南 往 北 方 向 行 駛 ， 行 經 該 街 道 242 號 前 ， 將 上 開 自 小 貨 車 停 進 該 處 騎 樓 後 ， 於 倒 車 離 開 時 ， 本 應 注 意 顯 示 倒 車 燈 ， 謹 慎 緩 慢 後 退 ， 並 應 注 意 其 他 車 輛 及 行 人 ， 竟 未 能 注 意 ， 貿 然 倒 車 ， 致 撞 及 張 麗 雲 所 騎 乘 之 車 牌 號 碼 c ##q ##7 - 921 號 機 車 ， 造 成 張 麗 雲 人 車 倒 地 並 往 前 滑 行 ， 因 而 受 有 膝 挫 傷 、 手 挫 傷 等 傷 害 （ 所 犯 過 失 傷 害 部 分 ， 未 據 告 訴 ） 。 [UNK] 趙 平 義 於 肇 事 後 ， 明 知 汽 車 駕 駛 人 ， 如 肇 事 致 人 受 傷 ， 應 即 採 取 救 護 或 其 他 必 要 措 施 ， 不 得 逃 逸 ， 竟 未 停 車 察 看 及 對 張 麗 雲 施 以 救 助 ， 或 採 取 其 他 必 要 措 施 ， 仍 逕 自 駕 車 逃 逸 。 嗣 經 路 人 記 下 車 號 ， 為 警 循 線 查 獲 。 [SEP]\n",
      "10/03/2020 18:06:42 - INFO - run_classifier -   input_ids: 101 6635 2398 5412 3176 3696 1751 8135 2399 128 3299 8124 3189 678 1286 125 3229 8113 1146 6258 8024 7690 7691 6722 4277 5998 4826 152 8159 118 12110 8161 5998 5632 2207 6515 6722 8024 3784 5637 1298 2356 472 1281 9423 6125 4507 1298 2518 1266 3175 1403 6121 7691 8024 6121 5195 6283 6125 6887 11056 5998 1184 8024 2200 677 7274 5632 2207 6515 6722 977 6868 6283 5993 7697 3559 2527 8024 3176 948 6722 7431 7274 3229 8024 3315 2746 3800 2692 7549 4850 948 6722 4236 8024 6346 2708 5227 2714 2527 6842 8024 699 2746 3800 2692 1071 800 6722 6739 1350 6121 782 8024 4994 3313 5543 3800 2692 8024 6530 4197 948 6722 8024 5636 3058 1350 2484 7927 7437 2792 7697 733 722 6722 4277 5998 4826 145 8326 8161 118 12253 5998 3582 6722 8024 6863 2768 2484 7927 7437 782 6722 948 1765 699 2518 1184 3998 6121 8024 1728 5445 1358 3300 5607 2919 1003 510 2797 2919 1003 5023 1003 2154 8020 2792 4306 6882 1927 1003 2154 6956 1146 8024 3313 3087 1440 6260 8021 511 100 6635 2398 5412 3176 5488 752 2527 8024 3209 4761 3749 6722 7690 7691 782 8024 1963 5488 752 5636 782 1358 1003 8024 2746 1315 2967 1357 3131 6362 2772 1071 800 2553 6206 2974 3177 8024 679 2533 6845 6871 8024 4994 3313 977 6722 2175 4692 1350 2205 2484 7927 7437 3177 809 3131 1221 8024 2772 2967 1357 1071 800 2553 6206 2974 3177 8024 793 6855 5632 7690 6722 6845 6871 511 1632 5195 6662 782 6250 678 6722 5998 8024 4158 6356 2542 5221 3389 4363 511 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/03/2020 18:06:42 - INFO - run_classifier -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/03/2020 18:06:42 - INFO - run_classifier -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/03/2020 18:06:42 - INFO - run_classifier -   label: 公共危險 (id = 2)\n",
      "10/03/2020 18:06:42 - INFO - run_classifier -   *** Example ***\n",
      "10/03/2020 18:06:42 - INFO - run_classifier -   guid: val\n",
      "10/03/2020 18:06:42 - INFO - run_classifier -   tokens: [CLS] 猶 不 知 緩 改 ， 自 105 年 4 月 1 日 下 午 6 時 許 至 同 日 晚 上 7 時 許 止 ， 在 臺 中 市 東 區 進 德 北 路 上 之 友 人 家 中 ， 飲 用 摻 加 米 酒 之 燒 酒 雞 後 ， 旋 於 同 日 晚 上 7 時 30 分 許 ， 騎 乘 車 牌 號 碼 000 - 000 號 普 通 重 型 機 車 上 路 。 嗣 於 同 日 晚 上 7 時 40 分 許 ， 途 經 臺 中 市 北 區 進 德 北 路 與 力 行 路 交 岔 路 口 時 ， 因 形 跡 可 疑 且 面 有 酒 容 ， 為 警 攔 查 後 ， 發 現 其 渾 身 酒 味 ， 遂 對 其 施 以 吐 氣 酒 精 濃 度 檢 測 ， 測 得 其 吐 氣 所 含 酒 精 濃 度 為 每 公 升 0 . 26 毫 克 ， 而 查 獲 上 情 。 [SEP]\n",
      "10/03/2020 18:06:42 - INFO - run_classifier -   input_ids: 101 4348 679 4761 5227 3121 8024 5632 8423 2399 125 3299 122 3189 678 1286 127 3229 6258 5635 1398 3189 3241 677 128 3229 6258 3632 8024 1762 5637 704 2356 3346 1281 6868 2548 1266 6662 677 722 1351 782 2157 704 8024 7614 4500 3046 1217 5101 6983 722 4240 6983 7430 2527 8024 3181 3176 1398 3189 3241 677 128 3229 8114 1146 6258 8024 7697 733 6722 4277 5998 4826 8241 118 8241 5998 3249 6858 7028 1798 3582 6722 677 6662 511 1632 3176 1398 3189 3241 677 128 3229 8164 1146 6258 8024 6854 5195 5637 704 2356 1266 1281 6868 2548 1266 6662 5645 1213 6121 6662 769 2264 6662 1366 3229 8024 1728 2501 6657 1377 4542 684 7481 3300 6983 2159 8024 4158 6356 3105 3389 2527 8024 4634 4412 1071 3954 6716 6983 1456 8024 6876 2205 1071 3177 809 1402 3706 6983 5125 4083 2428 3596 3947 8024 3947 2533 1071 1402 3706 2792 1419 6983 5125 4083 2428 4158 3680 1062 1285 121 119 8153 3690 1046 8024 5445 3389 4363 677 2658 511 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/03/2020 18:06:42 - INFO - run_classifier -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/03/2020 18:06:42 - INFO - run_classifier -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/03/2020 18:06:42 - INFO - run_classifier -   label: 公共危險 (id = 2)\n",
      "10/03/2020 18:06:42 - INFO - run_classifier -   ***** Running evaluation *****\n",
      "10/03/2020 18:06:42 - INFO - run_classifier -     Num examples = 134\n",
      "10/03/2020 18:06:42 - INFO - run_classifier -     Batch size = 2\n",
      "10/03/2020 18:06:52 - INFO - run_classifier -   ***** Eval results *****\n",
      "10/03/2020 18:06:52 - INFO - run_classifier -     eval_accuracy = 0.03731343283582089\n",
      "10/03/2020 18:06:52 - INFO - run_classifier -     eval_loss = 2.7967972292828915\n",
      "10/03/2020 18:06:52 - INFO - run_classifier -     global_step = 1446\n",
      "10/03/2020 18:06:52 - INFO - run_classifier -     loss = 2.7771626354747787\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "_uuid": "741b42a6de4d76a48001165d0efdb59a22f5e420",
    "id": "7IDATxIwuFyg",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "def predict(model, tokenizer, examples, label_list, eval_batch_size=128):\n",
    "    model.to(device)\n",
    "    eval_examples = examples\n",
    "    eval_features = convert_examples_to_features(\n",
    "        eval_examples, label_list, max_seq_length, tokenizer)\n",
    "    logger.info(\"***** Running evaluation *****\")\n",
    "    logger.info(\"  Num examples = %d\", len(eval_examples))\n",
    "    logger.info(\"  Batch size = %d\", eval_batch_size)\n",
    "    all_input_ids = torch.tensor([f.input_ids for f in eval_features], dtype=torch.long)\n",
    "    all_input_mask = torch.tensor([f.input_mask for f in eval_features], dtype=torch.long)\n",
    "    all_segment_ids = torch.tensor([f.segment_ids for f in eval_features], dtype=torch.long)\n",
    "    all_label_ids = torch.tensor([f.label_id for f in eval_features], dtype=torch.long)\n",
    "    eval_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "    # Run prediction for full data\n",
    "    eval_sampler = SequentialSampler(eval_data)\n",
    "    eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=eval_batch_size)\n",
    "\n",
    "    model.eval()\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    \n",
    "    res = []\n",
    "    for input_ids, input_mask, segment_ids, label_ids in eval_dataloader:\n",
    "        input_ids = input_ids.to(device)\n",
    "        input_mask = input_mask.to(device)\n",
    "        segment_ids = segment_ids.to(device)\n",
    "#         label_ids = label_ids.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "#             tmp_eval_loss = model(input_ids, segment_ids, input_mask, label_ids)\n",
    "            logits = model(input_ids, segment_ids, input_mask)\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "#         print(logits)\n",
    "        res.extend(logits.argmax(-1))\n",
    "#         label_ids = label_ids.to('cpu').numpy()\n",
    "#         tmp_eval_accuracy = accuracy(logits, label_ids)\n",
    "\n",
    "#         eval_loss += tmp_eval_loss.mean().item()\n",
    "#         eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "#         nb_eval_examples += input_ids.size(0)\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "#     eval_loss = eval_loss / nb_eval_steps\n",
    "#     eval_accuracy = eval_accuracy / nb_eval_examples\n",
    "#     loss = tr_loss/nb_tr_steps \n",
    "#     result = {'eval_loss': eval_loss,\n",
    "#               'eval_accuracy': eval_accuracy,\n",
    "#               'global_step': global_step,\n",
    "#               'loss': loss}\n",
    "\n",
    "#     output_eval_file = os.path.join(output_dir, \"eval_results.txt\")\n",
    "#     with open(output_eval_file, \"w\") as writer:\n",
    "#         logger.info(\"***** Eval results *****\")\n",
    "#         for key in sorted(result.keys()):\n",
    "#             logger.info(\"  %s = %s\", key, str(result[key]))\n",
    "#             writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n",
    "    return res\n",
    "    "
   ],
   "execution_count": 23,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "_uuid": "e20b72f980d3a0112a1b6ac1c67d0f1823c1f4d9",
    "id": "UOjUEa9_uFyj",
    "colab_type": "code",
    "outputId": "e710595f-04d5-4dbe-bea0-9d66834f7f5a",
    "executionInfo": {
     "status": "error",
     "timestamp": 1577895653868,
     "user_tz": -480,
     "elapsed": 608,
     "user": {
      "displayName": "曹錫璋",
      "photoUrl": "",
      "userId": "12089102222035734305"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 308
    }
   },
   "source": [
    "res = predict(model, tokenizer, test_examples, label_list)"
   ],
   "execution_count": 24,
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'None'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-24-194e0da1cc8f>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mres\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mpredict\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtokenizer\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtest_examples\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlabel_list\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-23-ebfd150d9bde>\u001B[0m in \u001B[0;36mpredict\u001B[1;34m(model, tokenizer, examples, label_list, eval_batch_size)\u001B[0m\n\u001B[0;32m      3\u001B[0m     \u001B[0meval_examples\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mexamples\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m     eval_features = convert_examples_to_features(\n\u001B[1;32m----> 5\u001B[1;33m         eval_examples, label_list, max_seq_length, tokenizer)\n\u001B[0m\u001B[0;32m      6\u001B[0m     \u001B[0mlogger\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0minfo\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"***** Running evaluation *****\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      7\u001B[0m     \u001B[0mlogger\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0minfo\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"  Num examples = %d\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0meval_examples\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\PycharmProjects\\Pytorch\\run_classifier.py\u001B[0m in \u001B[0;36mconvert_examples_to_features\u001B[1;34m(examples, label_list, max_seq_length, tokenizer)\u001B[0m\n\u001B[0;32m    258\u001B[0m         \u001B[1;32massert\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput_mask\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m==\u001B[0m \u001B[0mmax_seq_length\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    259\u001B[0m         \u001B[1;32massert\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msegment_ids\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m==\u001B[0m \u001B[0mmax_seq_length\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 260\u001B[1;33m         \u001B[0mlabel_id\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mlabel_map\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mexample\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlabel\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    261\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mex_index\u001B[0m \u001B[1;33m<\u001B[0m \u001B[1;36m5\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    262\u001B[0m             \u001B[0mlogger\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0minfo\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"*** Example ***\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyError\u001B[0m: 'None'"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "_uuid": "1523e50eb6a113b13a4de1f3926b73e62ba36e97",
    "id": "5UGha3kbuFym",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "label_list"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "_uuid": "bf78ac2d2be4dbb4b63f69c5e82c34351df6c8df",
    "scrolled": true,
    "id": "L0N-s2EXuFyq",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "set(res)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "_uuid": "bab40a86d6cd07aa1456f37fbdc0d26f94000309",
    "id": "PRn9RaN9uFyt",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "predict(model, tokenizer, test_examples[:10], label_list)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "_uuid": "79c0163e94d9c5f6b1e5ae4a71ebf65f4ff58feb",
    "id": "aPmSIAaYuFyw",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "cat_map = {idx:lab for idx, lab in enumerate(label_list)}\n",
    "res = [cat_map[c] for c  in res]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Nv4xQ22PLV9-",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "test.tail()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "_uuid": "dcc715e51af5120090589a9b91169bbf8d01d705",
    "id": "6X_wIj2NuFy0",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "#　For Submission\n",
    "\n",
    "test['label'] = res\n",
    "\n",
    "\n",
    "submission = test\n",
    "\n",
    "submission.columns = ['titletext', 'label']\n",
    "submission.to_csv('./CQA/data/submission.csv', index=False)\n",
    "submission.head()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "xsAKHbX2NM-8",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "test_r = pd.read_csv(TEST_CSV_PATH)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Z8t9O8urNVGm",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "test_r.head()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "DK0ycTv_Nmdy",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "test_m = pd.merge(test, test_r, left_index=True, right_index=True)\n",
    "test_m = test_m[[\"titletext_x\", \"Category\", \"label\" ]]\n",
    "test_m.columns = ['titletext', 'Category', 'label']\n",
    "test_m.to_csv('./CQA/data/submission_compare.csv', index=False)\n",
    "test_m.head()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "IRxz7wpVNmRp",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "test_m.head()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "PgK2u_HjN92K",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "test_fail = test_m[test_m[\"Category\"]==test_m[\"label\"]]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "oNbrqWOuOhhI",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "test_fail.count()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0nS-UY1yUBau",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "test_fail.count()"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}